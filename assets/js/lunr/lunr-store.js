var store = [{
        "title": "Architecture & Design",
        "excerpt":" ","categories": [],
        "tags": [],
        "url": "/areas/arch/",
        "teaser": "/assets/images/cards/codememo.png"
      },{
        "title": "Build",
        "excerpt":" ","categories": [],
        "tags": [],
        "url": "/areas/build/",
        "teaser": "/assets/images/cards/codememo.png"
      },{
        "title": "DevOps",
        "excerpt":" ","categories": [],
        "tags": [],
        "url": "/areas/devops/",
        "teaser": "/assets/images/cards/codememo.png"
      },{
        "title": "Organization & Culture",
        "excerpt":" ","categories": [],
        "tags": [],
        "url": "/areas/org/",
        "teaser": "/assets/images/cards/codememo.png"
      },{
        "title": "Test",
        "excerpt":" ","categories": [],
        "tags": [],
        "url": "/areas/qa/",
        "teaser": "/assets/images/cards/codememo.png"
      },{
        "title": "Version Control",
        "excerpt":" ","categories": [],
        "tags": [],
        "url": "/areas/vcs/",
        "teaser": "/assets/images/cards/codememo.png"
      },{
        "title": "Commits are tied to tasks",
        "excerpt":"Every time the code changes, it traces back to some task being done. Think about it; Who’s feeding the process? How are tasks being created, prioritized and executed?   When a code change cannot be tied to a specific task, it’s likely something that has bypassed the normal chain of command. Somebody is working outside protocol.   Make all code changes part of an overall plan - simply pair commits with tasks. Besides generating valuable traces that can later be used in an audit or documentation of your trail, it also enables you to track the pace of the team and maintain a burn-down chart.  ","categories": [],
        "tags": [],
        "url": "/commits-are-tied-to-tasks/",
        "teaser": "/assets/images/cards/commits-are-tied-to-tasks.png"
      },{
        "title": "Pristine integration branch",
        "excerpt":"To integrate means to merge your code on to the same branch as the one your colleagues are working on.  So obviously if your code breaks something you are potentially jeopardizing the work space - and pace - of your team mates as well. To have a pristine integration branch means that it is buildable at all times.   Code should be verified through some kind of toll-gate criteria, before it’s accepted on to the integration branch. Anything that doesn’t meet the toll-gate criteria is rejected and will not enter the mainline. It is simply impossible for a developer to break the build.  ","categories": [],
        "tags": [],
        "url": "/pristine-integration-branch/",
        "teaser": "/assets/images/cards/pristine-integration-branch.png"
      },{
        "title": "Release train branching strategy",
        "excerpt":"The release train branching strategy is similar to what is sometimes referred to as late branching or trunk based development. Essentially it implies that in your entire branch tree there is only one branch that is meant to be long-lived.   Consequently, there is only one way you can contribute to a product and have your code released, and that is to deliver your code to the mainline.   In a release train strategy, the mainline is on it’s way to production all the time, and therefore anything that isn’t meant for production as soon as possible, shouldn’t be delivered to the mainline.  ","categories": [],
        "tags": [],
        "url": "/release-train-branching-strategy/",
        "teaser": "/assets/images/cards/release-train-branching-strategy.png"
      },{
        "title": "Use distributed VCS",
        "excerpt":"Distributed version control systems are a faster, modern alternative with a healthy community. Due to its distributed nature, switching to git opens up many doors to automation. e.g.: It allows for automation tools to work in a local repository without jeopardizing the mainline.   Keep your project’s history clean and understandable. Make it easier to find specific commits and for others to review.  When finishing up work in your short-lived branches, clean up your local commit history before merging back into the integration branch.  ","categories": [],
        "tags": [],
        "url": "/use-dvcs/",
        "teaser": "/assets/images/cards/use-dvcs.png"
      },{
        "title": "Version numbers matter",
        "excerpt":"Versioning schemes are a powerful tool.   They give you a quick and accurate reference to when, where and by whom something was made, what it’s compatible with, etc. It is the name of your release, the identifier of your component, the passport of your product.   Implement a well-defined versioning scheme for your components and releases.  ","categories": [],
        "tags": [],
        "url": "/version-numbers/",
        "teaser": "/assets/images/cards/version-numbers.png"
      },{
        "title": "Artifacts are managed",
        "excerpt":"An artifact is the output derived from your build process.   Sadly, artifacts are often built whenever they’re needed. A lot of the builds just build that which has been built before.   Even though this could be about contributing to avoiding the environmental crisis, it is also justifiable to save and manage artifacts simply to save wait-states and bottlenecks in the software development process.   Stop building things that haven’t changed and start reusing colleagues’ artifacts - install an artifact management system.  ","categories": [],
        "tags": [],
        "url": "/artifacts-are-managed/",
        "teaser": "/assets/images/cards/artifacts-are-managed.png"
      },{
        "title": "Automated builds",
        "excerpt":"When code changes are committed to the repository, your CI server kicks in automatically and starts a build.   It might not even be an actual build, it can be any kind of automated action that is part of a verification process implied in the project’s “definition of done”.   If a build step fails, the developer is notified directly so he can start working on fixing the issue immediately. The shorter the feedback loop, the better.  ","categories": [],
        "tags": [],
        "url": "/automated-builds/",
        "teaser": "/assets/images/cards/automated-builds.png"
      },{
        "title": "Automated release notes",
        "excerpt":"Whenever you ship a new release you probably need a release note, a report listing the new version number, fixed issues, new features…   Why write all that manually? By building up your traces and recording your trails as you work, you can pull all this information out of your backend automatically.   You’ve written your last release note. From here on out it’s release notes as code.  ","categories": [],
        "tags": [],
        "url": "/automated-release-notes/",
        "teaser": "/assets/images/cards/automated-release-notes.png"
      },{
        "title": "Delivery pipeline",
        "excerpt":"Split up your builds and verifications into a pipeline consisting of multiple stages. Use this approach to keep your builds as fast as possible, your feedback loop as short as possible and your developers notified as quickly as possible despite having long-running builds.   In your pipeline, each step provides more confidence in your code than the previous one.  ","categories": [],
        "tags": [],
        "url": "/delivery-pipeline/",
        "teaser": "/assets/images/cards/delivery-pipeline.png"
      },{
        "title": "Full traceability",
        "excerpt":"Take an arbitrary piece of hardware; As part of the Product Lifecycle Management the manufacturer has a complete trace to the individual versions of all the components that went into it.   Application Lifecycle Management means that you apply the same approach to software. You trace everything.   The requirements that created the tasks, the commits that resolved them, the compiler that built them, the tests you ran, the environment you ran them in, the test results and so on. When the software reaches production, you know the specifics of everything that created it.  ","categories": [],
        "tags": [],
        "url": "/full-trace/",
        "teaser": "/assets/images/cards/full-trace.png"
      },{
        "title": "Code metrics",
        "excerpt":"Analyzing your code is cheap but valuable. Scouring your code and producing interesting metrics helps you keep check on all kinds of creeping issues.   Static code analysis, style checkers, cyclomatic complexity, code coverage and scanning for FIXMEs and TODOs are all examples of metrics that help you keep a watchful eye on codebase evolution. Adding thresholds to these measures as part of your verification protects it from corruption.   Monitor improvements but don’t waste time reaching arbitrary targets.  ","categories": [],
        "tags": [],
        "url": "/code-metrics/",
        "teaser": "/assets/images/cards/code-metrics.png"
      },{
        "title": "Dependencies are managed",
        "excerpt":"All software has dependencies; You may be using third party technology or you have a lot of individually released microservices, frameworks or libraries in your system architecture.   Make sure there are no moving targets and don’t rely on someone else’s master, latest  or stable release. Cache everything you need in your own registry.   Optimize your link processes to use cached dependencies when available, optimize your compile processes to feed the registry when new versions are created, so others can benefit from them.  ","categories": [],
        "tags": [],
        "url": "/dependencies-are-managed/",
        "teaser": "/assets/images/cards/dependencies-are-managed.png"
      },{
        "title": "Full audit trail in production",
        "excerpt":"When something goes south - it’s usually in the production environment - where you don’t have access to debug or profiling information.   Design you code, so it can produce an audit trail - a complete profile of states, sequences, data in and out. That should give you clues, when you try to do your code-scene forensics.   At least you’ll get some clues on how to reproduce the error in your development environment.  ","categories": [],
        "tags": [],
        "url": "/full-audit-trail/",
        "teaser": "/assets/images/cards/full-audit-trail.png"
      },{
        "title": "Individually releasable components",
        "excerpt":"This principle lends itself to common coding standards of high cohesion and low coupling.   Break down your monolith, identify all the nuts and bolts in your architecture that produce an actual artifact - like a binary executable from compilation or any other kind of package.   Make these components self-contained with it’s own definition of done, it’s own pipeline, it’s own interface - it’s own release process.   Treat it as inventory and manage your dependencies.  ","categories": [],
        "tags": [],
        "url": "/individually-releasable-components/",
        "teaser": "/assets/images/cards/individually-releasable-components.png"
      },{
        "title": "Testable code",
        "excerpt":"Whether or not a particular code snippet gets tested is often a matter of how easy it is to test.   Organize your code in easily accessible features. Make each feature available through one interface only.   Since the feature is only available through one interface, it’s safe to consider it tested, when you have massaged it.   At the end of the day, more code gets tested.  ","categories": [],
        "tags": [],
        "url": "/testable-code/",
        "teaser": "/assets/images/cards/testable-code.png"
      },{
        "title": "Automated deployment",
        "excerpt":"The ability of a release to be deployed is such an essential part of the delivery that the developer is expected to take full responsibility for this process.   The deployment should be automated because it’s a task that needs to be carried out often and is not necessarily trivial.   You want your deployment as code.  ","categories": [],
        "tags": [],
        "url": "/automated-deployment/",
        "teaser": "/assets/images/cards/automated-deployment.png"
      },{
        "title": "Infrastructure as code",
        "excerpt":"Every aspect of your entire development and release process can be traced back to some kind of version controlled code.   This can range from the versions of your dependencies to the configuration of your CI server pipeline.   In this context as code means that it’s persisted in files, it’s syntax can be checked, it has semantic meaning, it’s version controlled and can be executed.  ","categories": [],
        "tags": [],
        "url": "/infrastructure-as-code/",
        "teaser": "/assets/images/cards/infrastructure-as-code.png"
      },{
        "title": "Live monitoring and feedback",
        "excerpt":"Your software is in production, but how is it doing?  You want to have insight into the runtime health of your system.   This includes easy access to runtime statistics such as feature usage, transaction throughput and error situations to ensure the service level.  In addition, access to environment health like disk and memory usage, cpu load.   Bonus points if your system can alert you before an error occurs.  ","categories": [],
        "tags": [],
        "url": "/live-monitoring-and-feedback/",
        "teaser": "/assets/images/cards/live-monitoring-and-feedback.png"
      },{
        "title": "Access to production-like environments",
        "excerpt":"Both successful deployment and functional testing are often considered part of the definition of done.   When we´re building continuous delivery pipelines it’s because we want our developers to have access to, and to execute, the full definition of done.   Use simulators, emulators, containers and VMs to verify your definition of done and ensure stable releases. The closer to production your testing environment, the more confidence you can have in your changes.  ","categories": [],
        "tags": [],
        "url": "/on-demand-environments/",
        "teaser": "/assets/images/cards/on-demand-environments.png"
      },{
        "title": "One Team",
        "excerpt":"Groups of professionals contributing to the same product that are working in isolated silos and not talking to each other is not helping your project one bit.   Break down these silos by having contributors talk to each other and involving them in the bigger picture.   The term full-stack-developer is used to describe a developer whom is fully capable of doing whatever is required. This is the essence of not working in silos.  ","categories": [],
        "tags": [],
        "url": "/one-team/",
        "teaser": "/assets/images/cards/one-team.png"
      },{
        "title": "Agile process",
        "excerpt":"Agile processes defies phases in the software development process. The 1st principle in the Agile Manifesto refers to continuously delivering software.   An agile approach is not a small waterfall or a interactive and incremental process speeded up to 14 days intervals. It literally has no phases - only continuous integration and continuous delivery with focus on minimizing work in progress and get the right thing done - at a constant pace.  ","categories": [],
        "tags": [],
        "url": "/agile-process/",
        "teaser": "/assets/images/cards/agile-process.png"
      },{
        "title": "Buy-in from management",
        "excerpt":"Initiatives, new tools and approaches are often of natural interest to developers and they might experiment and research without explicitly being told to do so.   But Continuous Delivery is a paradigm that strives to build quality into the product rather than gluing it on afterwards.   Transitioning is going to take time. It needs planning. It needs prioritization. It needs funding. You need a road map. Continuous Delivery is not a quilted patchwork. Be sure to make it a corporate thing, not just a neat idea.  ","categories": [],
        "tags": [],
        "url": "/buy-in-management/",
        "teaser": "/assets/images/cards/buy-in-management.png"
      },{
        "title": "Designated roles",
        "excerpt":"Shared responsibility often leads to misunderstandings. When the people involved  rely on the others to manage that responsibility.   Even if the responsibility is assigned to a role, and that role is given to one person only, it’s often the case that the person hasn’t allocated time to actually perform the duties.   Every process, that’s required, needs to be assigned to a role and that role needs to be assigned to a person, who is actually expected to responsibly spend time on performing these duties.  ","categories": [],
        "tags": [],
        "url": "/designated-roles/",
        "teaser": "/assets/images/cards/designated-roles.png"
      },{
        "title": "Explicit knowledge transfer",
        "excerpt":"The bus factor measures how many people in your corporation need to be run over by a bus before you go out of business. If you have a key player who’s indispensable, then your bus factor is 1.   To raise the bus factor, you must make sure that important knowledge is shared and accessible to whoever needs it.   Don’t document your processes to the brink of boredom or maintain an internal wiki the size of Wikipedia itself. Build a learning organization that encourages people to share with colleagues, allocates time for research, designs for change and accepts automation as documentation.  ","categories": [],
        "tags": [],
        "url": "/knowledge-transfer/",
        "teaser": "/assets/images/cards/knowledge-transfer.png"
      },{
        "title": "Tasks are groomed",
        "excerpt":"Assignments should be prepared for working before they qualify as actual tasks. The goal of a task must be known to the person who is implementing it.   If a task is ambiguous it can not be estimated, and if it can not be estimated, it can not be prioritized.   If a task doesn’t have a clear definition of done, then it should be time-boxed.  ","categories": [],
        "tags": [],
        "url": "/tasks-are-groomed/",
        "teaser": "/assets/images/cards/tasks-are-groomed.png"
      },{
        "title": "Adaptive test suites",
        "excerpt":"When your test cases are self-contained with individual setups and tear-downs and they trace to related functions and features, you are able to analyze a given change set, place it in context of a limited amount of features and derive its relevant test cases.   Then you can construct an adaptive test suite on the fly and execute that on a production-like environment.   By running a small and relevant subset of functional tests, you can add functional testing to the short feed-back loop.  ","categories": [],
        "tags": [],
        "url": "/adaptive-test-suites/",
        "teaser": "/assets/images/cards/adaptive-test-suites.png"
      },{
        "title": "Automated functional tests",
        "excerpt":"In a functional test, you test the features that the system offers as a whole, seen from the end user’s perspective.   In the previous millennium such a test would be planned by a person with domain knowledge, then executed prior to every release by testers performing manual operations based on written instructions.   A more contemporary strategy is to have the person with domain knowledge manage a team of developers, who are actually implementing the tests as code, and then give the software developers access to execute these test in their production-like environments.  ","categories": [],
        "tags": [],
        "url": "/automated-functional-tests/",
        "teaser": "/assets/images/cards/automated-functional-tests.png"
      },{
        "title": "Maintain test data",
        "excerpt":"Management and maintenance of your test data is considered part of your Quality Assurance strategy. Your test data is versioned and stored as an artifact.   This implies that you separate your test data from the actual tests, which in turn comes with the benefit of easily running test suites with different, versioned data sets.   Test suites becomes self-contained, each with their own easy reproducible setup and tear-down steps, something that will later enable you to run your test suites independently of each other - maybe even selected on output from previous verification steps in your pipeline.  ","categories": [],
        "tags": [],
        "url": "/maintain-test-data/",
        "teaser": "/assets/images/cards/maintain-test-data.png"
      },{
        "title": "Test in production",
        "excerpt":"A word of precaution; testing in production is not to be confused with releasing untested code.   It starts with acknowledgement that all serious problems are discovered in production and occurred because unforeseen things happened.   Deliberately go to your production environment and do unforeseen things like turn off a server, kill a process, pour coffee on your keyboard, upgrade a service during high-load.   If your system is built to survive it, then it should! You’re only sure it will if you (dare) test it.  ","categories": [],
        "tags": [],
        "url": "/test-in-production/",
        "teaser": "/assets/images/cards/test-in-production.png"
      },{
        "title": "Unit testing, mocks, stubs and proxies",
        "excerpt":"Unit tests are used to test the semantics of your code; To verify it works as expected and keeps working as expected through changes.   Unit tests are light-weight and fast. Don’t get tangled up in hard-to-handle dependencies such as loading databases or instantiating long sequences of objects before you get to the actual testing, use mocks and stubs to simulate your first order dependencies. Or use Proxies to have non-local collaborators contribute to your test.   A unit test is quick to execute and it should be executable in the context of your development environment.  ","categories": [],
        "tags": [],
        "url": "/unit-test/",
        "teaser": "/assets/images/cards/unit-tests.png"
      },{
        "title": "Continuous Delivery Assessment",
        "excerpt":"                                      Novice                            Beginner                            Intermediate                            Advanced                            Expert                                                         Build                                                                                                                                                                                 Automated builds                                                                                                                                                                                                                                               Artifacts are managed                                                                                                                                                                                                                                               Automated release notes                                                                                                                                                                                                                                               Full traceability                                                                                                                                                                                                                                               Delivery pipeline                                                                                                                                    Test                                                                                                                                                                                 Unit testing, mocks, stubs and proxies                                                                                                                                                                                                                                               Automated functional tests                                                                                                                                                                                                                                               Maintain test data                                                                                                                                                                                                                                               Adaptive test suites                                                                                                                                                                                                                                               Test in production                                                                                                                                    Version Control                                                                                                                                                                                 Use distributed VCS                                                                                                                                                                                                                                               Commits are tied to tasks                                                                                                                                                                                                                                               Release train branching strategy                                                                                                                                                                                                                                               Version numbers matter                                                                                                                                                                                                                                               Pristine integration branch                                                                                                                                    DevOps                                                                                                                                                                                 One Team                                                                                                                                                                                                                                               Automated deployment                                                                                                                                                                                                                                               Access to production-like environments                                                                                                                                                                                                                                               Infrastructure as code                                                                                                                                                                                                                                               Live monitoring and feedback                                                                                                                                    Architecture &amp; Design                                                                                                                                                                                 Code metrics                                                                                                                                                                                                                                               Testable code                                                                                                                                                                                                                                               Dependencies are managed                                                                                                                                                                                                                                               Individually releasable components                                                                                                                                                                                                                                               Full audit trail in production                                                                                                                                    Organization &amp; Culture                                                                                                                                                                                 Agile process                                                                                                                                                                                                                                               Buy-in from management                                                                                                                                                                                                                                               Tasks are groomed                                                                                                                                                                                                                                               Designated roles                                                                                                                                                                                                                                               Explicit knowledge transfer                                                                                                             sample     https://codemamo.lakruzz.com                     November 24, 2016     Author: Lars Kruse     Co-authors:         ","categories": [],
        "tags": [],
        "url": "/customer_details/sample/report-parts/front/",
        "teaser": null
      },{
        "title": "Context",
        "excerpt":"This report summarizes the findings of a series of workshop conducted in relation to the CoDeMeMo performed for Your Awesome Company on 2024-04-07 00:00:00 +0000.   Here follows a short intro to Your Awesome Company    The Continuous Delivery Assessment   The interviews and workshops related to the CoDeMeMo has the purpose of getting a complete overview of the organization with specific focus on software development, the tool stack that surrounds and supports it and the processes used to develop, verify, deploy, test and release software.   The goal of the interviews and workshops is two-fold:      To raise awareness in the software development team of some of the best practice methods, tools and processes available for improving efficiency and producing higher quality software.   To outline a roadmap as a practical guide to achieving this goal.   Awareness is achieved by conducting a considerable part of the interviews and workshops and facilitated discussions with the developers in the software team.   The roadmap is manifested in the report you are currently reading.   This report   The report is structured around the Continuous Delivery Metric Model which was presented to all participants during the workshop. The chapters in the report reflect the focus areas in the model.   Throughout the report, findings and observations are given their own section header and paragraphs under each section are marked with an up-arrow  indicating that it contains a recommended action mitigating an issue.    Mitigation’s are tagged with a prioritization (M, S, C or W in parentheses). These letters represent the authors opinionated recommendation of priority for the suggested actions.   The lettering refers to the MoSCoW principle for prioritization:      Must have A condition that must be satisfied in the final solution for the solution to be considered a success.   Should have A condition that should be included in the solution if it is possible. A critical condition indeed but not discriminating for success if left out.   Could have A condition that could be included if time and resources permit. It is considered desirable but not strictly necessary for success.   Won’t have (not used in this report) A condition that is not currently relevant and which will not be implemented but could potentially become relevant in a future scope.   The prioritization that we have come up with is meant to be a catalyst for your internal discussions - you may not agree with our priorities in which case you should obviously go with your own.   Each section is designed as a standalone piece so that the suggested remedies and improvements can be applied independently. This approach should enable you to copy most sections marked with  directly into JIRA and prioritize them. All you need to do hereafter is to groom the task, estimate the work, and put it on your backlog.     ","categories": [],
        "tags": [],
        "url": "/customer_details/sample/report-parts/context/",
        "teaser": null
      },{
        "title": "Findings",
        "excerpt":"The findings and recommendations have mainly been picked up throughout the interviews and workshops. We conducted this part as targeted group interviews focusing on the 6 main areas of the metric model.   On the first day, after the initial Continuous Delivery presentation, we had an exercise where all attendees together were asked to illustrate the process from the point where a requirement or a change request arrives until the implemented result can be accessed by the end-users.   The participants were also equipped with stickers with 6 different pictograms representing: Wait state, unplanned work, manual work, conflicts, queued work and anything that needs repair.   The result was discussed during the workshop and then used as a reference point during the various discussions on the following day.     Build                                                                                                                                                                                                                                                                                                                                                                                   Note: The following is a curated mix of illustrative findings from various anonymized reports   It builds on my machine   Builds and tests run on individual developer’s self-maintained machines and workspaces, as no standard image for this exists. This results in an undesirable “Well, it works on my machine” scenario. One developer can have a hard time redoing another developer’s work. The team suffers from a lot of false negatives. Builds fail often. Not because the code is broken, but because the build environment is.   Use a clean standardized image for the toll gate job   Code changes should be built and tested on a standard and clean image. The image should be maintained by the team. This prevents local configuration from tampering with the builds, making them deterministic and consistent.   Developers use reference image   Ideally, there should be no difference between a developer’s environment and that of the toll gate. This is achieved by making it easy for developers to spin up the standard image to run their builds, allowing them to easily produce the same result as the toll gate job.   Final delivery to customer is not automated   The image given to the customer is generated before final test in a non-reproducible way. It is built manually based on an already running device with an other, older image. It is modified by manipulating files within the device before a final image snapshot. Conceptually the image could be broken during the creation process.   Automate image creation   Any release artifact should always be possible to reproduce without any doubt. It should be scripted 100%. Programmable and immutable.   Make the image generation stateless   A product to a customer should always be generated from scratch in order not to propagate error prone states. It should also always be reproducible through a declarative pipeline.   Automatically test the image   The automatically generated image is tested as part of a pipeline. You want to extend the pipeline as far down the release process as possible.   Release process is, to a large extent, manual   The majority of the release process located in the Front Office is a manual one, which is costly, lengthy and error prone, e.g. the image creation, release notes, support site updates, etc.   Design the release process for automation   It is important that each step is designed for automation. Make sure the physical resource are available and enabled for automatic deployment. It relates to servers, but also devices that they can be claimed, started and the execution evaluated.   Script each single step to enable automatic execution   Identify the process steps that are manual, entirely or even partially so. Automate step by step and integrate them into the pipeline.   Build script is an plaster on the monolith   The build script is created to handle the monolith by plastering each of the subsystem builds into a single step. The different subsystems have different build technologies (e.g. Maven and Scons), but in order to make it all work as a monolith it is wrapped with shell script.   Split the monolith and add dependency management of the artifacts   Splitting the monolith into subsystems and implementing true dependency management makes the build script obsolete. It simply vanishes. Please refer to the architecture cards for more information.   Localization files are introduced late in the release process, leading to unexpected UI changes   Localized text in the UI can cause the UI controls to change in some unexpected manner, e.g. resizing of buttons due to translated texts being longer than their original. Commit-stop is the point where no further features, and thus no new text, gets added. Translation can take up to one month and arrives fairly late in the release process.   Create requirements for localization subcontractor   Create criteria or a framework that helps these text translators adhere to font type, font size, etc. This gives them the opportunity to react early and discuss with you any problematic translations.   This will minimize, or even completely avoid, nasty surprises near the end of the release process.   Manual process to create a buildable workspace   It is not possible to start building from a clean checkout. You have to checkout two repositories and create a symlink in order to be able to build the software. It is a hidden dependency.   Make the ‘link’ permanent by using a explicit dependency   Create a ‘thin shell’ repository which contains the dependency between these repositories. In SVN it is called an external, in Git it is called submodules. You could also pull the dependencies from Artifact Management system if the repositories have their own pipeline and definition of done. The implementation depends on the technology. The important point is the dependencies are managed and versioned.   The pipeline scripts contain domain specific logic   Your pipeline scripts are bloated with domain specific logic and setup. Much of the script relates to setting up environments and requirements to build the software. The pipeline scripts become large and unmanageable.   Keep domain logic out of pipeline scripts   Consider your pipeline definition as configuration rather than a script to help keep it lean. Avoid building logic into the pipeline script that has nothing to do with either CI system’s interface, execution or reporting. Remember that all the logic you put in these scripts is also something developers would need to run at their desk, should they want to recreate runs locally. The pipeline script makes for a poor place to document these setup steps                                                                                                                                                                                                                                                                                                                                                                             Note: The following is a curated mix of illustrative findings from various anonymized reports   Artifact management system configuration not optimal   The setup used in Artifact management is not as the tool is designed and intended for. The intended root structure is projects and on top of that a semantic structure with package and semantic version below. The current setup abuses the root structure as it is both the project and version which means that newcomers are confused and the root structure is polluted and grows as more releases are produced. The structure you’ve set up requires advanced logic in your build scripts rather than just simple SemVer logic.   Implement best practice structure   Store the artifacts in a hierarchical tree structure. It should also be done for non-Java artifacts if there is no explicit reason to not do so. Always respect the intent of a tool. Read the manual again, follow some of the tutorials for the tool. It will make the automation of the dependency management less complex if there is a consistent structure for addressing the artifacts.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Note: The following is a curated mix of illustrative findings from various anonymized reports   Release Notes are manually created   The release notes are handled in Confluence. It is a manual process and it is time consuming. The Jira issues are extracted and post-processed for the final release note. It is glue-on quality - not built-in quality. The release note cannot be generated based on a meta-system like Jira alone. It does not have the source revision time record. Jira issues and releases in Jira do not understand relationship of changes in the SCM system.   Design your Jira issues for data extraction   It is important to configure you schema in Jira, so it stores all needed information about the logical change. Add a status which tells if it should be listed in the release note. Not every task should be listed in a release note that the end user sees. Add a field for end-user description which can be extracted if public.   Generate different release note for different stakeholders   The different stakeholders will have different needs. At least it makes sense to have two generated release notes. One for R&amp;D which is used for meta data quality checks and used to monitor ‘release-readiness’. The second is a public release note which is reporting what external stakeholders need to know. Meaning, which features have been implemented and which publicly known errors have been corrected.   Create your release note as a record of your SCM system   The only correct place to initialize your release note is from your SCM system as it is ‘the’ one and only truth about this particular revision compared to previous revision/releases.   Use a release note generator based on SCM system (M)   Given that the systems and process are designed for automation the report can be generated. Any changes to data for the release note is changed/added/removed at the root source of the information. This means that the release note can be regenerate based on the updated data.   Add the generator to the pipeline   Add the release note generation to the pipeline. It then becomes artifact as any other product generated as part of the pipeline. It will be an important artifact for the tester and product owner as it shows exact progress of the software for later promotion decisions on this particular revision.   Use the R&amp;D release note as a consistency check (S)   As part of the release gate it is possible to monitor the release readiness where the release DoD is programmed/implemented. This is to harvest traceability and use it as a consistency check.   The consistency checks could look something like the following      Do all SCM change-sets have a related Jira issue   Are all the Jira issues in a correct status   Do we have all the changes related to all the Jira issues for this release   Do the change sets, based on the Jira issues, have the needed reviews in the correct status   Security review process and documentation is manual   The security review is needed for the safety reaction of the device. Changes to these algorithms are related to a special and mandatory review that needs to be documented and signed-off as part of the release process. The review is not tool-aided and is currently documented in the Jira issue. It is a manual process to gather the reviews to create the document that is signed by the security-officer.   Implement a solid review process and automatically generate the documentation   It is advised to implement Crucible/FishEye(already available) as the front-end for the review. It handles the statuses of the reviews. Given the data and status is available in the Atlassian suite it is possible to auto generate the review report ready for signature.     An option is to use PAC as a foundation, it’s an open source tool that can be extended to also generate the review report.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Test                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Version Control                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           DevOps                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Architecture &amp; Design                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Organization &amp; Culture                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           ","categories": [],
        "tags": [],
        "url": "/customer_details/sample/report-parts/findings/",
        "teaser": null
      },{
        "title": "Scores",
        "excerpt":"During the individual workshops for each area, we do an exercise where the participants evaluate and score the five cards that belong to the area. Through six workshops over two days we ended up scoring all 30 cards in the model.   The participants were split into smaller groups of 3-4 people, and each group was given one card to score. The exercises was to discuss the best-practice described on each card and score them in terms of the four gauges throughput, feedback, payback and simplicity.   After scoring the cards each group presented their scoring briefly, and the rest of the group had the option to affect or discuss it, if they found a need for it.     Throughput, feedback and payback are added together and multiplied by simplicity to calculate the total score on each card.   All the cards that got a score are listed in the following:                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Artifacts are managed                                                               Throughput                      Feedback                          Payback                            Complexity                                       Score                 6                                                               Are you building that over and over and over again?                                                                                Build                                                        Beginner                                                                                                                                                                                    Automated builds                                                               Throughput                      Feedback                          Payback                            Complexity                                       Score                 7                                                               Yes. But it doesn't build on THAT machine.                                                                                Build                                                        Novice                                                                                                                                                                                    Automated release notes                                                               Throughput                      Feedback                          Payback                            Complexity                                       Score                 5                                                               If it's boring - automate it. Release notes are boring.                                                                                Build                                                        Intermediate                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Note: the following is a sample analysis from an anonymous client   Analysis   The majority of the cards received some type of score in the value gauges. With the exception of the following      Automated release notes - received zero on payback   Test in production - received zero on throughput   Commits are tied to tasks - received zero on payback   Version numbers - received zero on throughput   It could be beneficial to revisit the cards, which you scored a zero in one of the value gauges, after reading this report. It might have given you inspiration to find the benefits from a holistic view.   Low hanging fruits   The scoring algorithm favors the simplicity gauge, putting easy-to-implement solutions higher up on the list.   This is a deliberate choice, as it helps identify the aptly named “low-hanging fruits” - ideas that should provide easy wins with high value. These are perfect issues to get started with as the quick gains often help you down the road and help motivate the organization to tackle its bigger, harder-to-solve pains.   Looking at your self-assessment, a few cards stood out:      Buy-in from management   Pristine integration branch   Release train branching strategy   Buy-in from management scored a 24 in the Organization and Culture category. You have evaluated this quite high in throughput and feedback. You gave it two stars for payback and feel it is simple to implement. We agree with this self assessment. We feel this is directly related to how you are going about paying off technical debt and implementing tasks to improve the way you work. Not so much as to whether you are allowed to. More about the approach of making it visible, organizing it better and planning it. This score is only a few points from being the ultimate low hanging fruit.   Pristine integration branch scored a 21 in the Version Control category. You have evaluated this as something that is easy to implement but gives fairly high value in feedback and throughput while affecting the architecture quite heavily. This is a good view, and to some extent, will not be the most difficult task of the results from this assessment. It might be a little more difficult than you anticipate. You need to align the definition of done across the teams and get the tooling, a DVCS for example, in place before this can be achieved effectively. Keep in mind that you want to make sure you do not impede the pace of the team with the automated toll gate. This could be slightly more complex than at first glance due to build and architectural dependencies.   Release train branching strategy scored an 18 in the Version Control category. This was assessed as being easy to implement, while giving a high value of throughput and fairly high value on feedback. It is interesting is that you gave it only one star for Payback. If you keep in mind the requirements a release train will impose on test-ability, trace-ability and maintain-ability, you may find that the value return is bigger here than you initially believe.   Falling back to the cards that score high with two stars in simplicity we get:      Code metrics - scored a 16 in Architecture and Design category.   Dependencies are managed - scored a 16 in Architecture and Design category.   The code metrics are a sure win and, in reality, should be part of your toll gate to get in to the integration branch. It can, generally, be kept fast and give definitive feedback.   The dependency management, on the other hand, will be pretty heavily related to what you already have implemented in this area. We know you have some dependency management from Maven. But what about the other technologies in play? How about an aligned version strategy that will support dependency management of releases in a good way? There are some rather hard relations to the architecture across the technologies here. This might, in reality, be a two in simplicity instead of a one?   Biggest blockers   Another thing we look for are your major pain points.   Judging the self-assessment scores, we now look further down the list for cards that were judged as very hard to implement but giving huge benefits when done.   These are cards that score high in throughput, feedback and payback but only a one in simplicity - indicating that they are difficult to implement.   There are three cards that stand out here.      Explicit knowledge transfer   Delivery Pipeline   Testable code   The Explicit knowledge transfer scored a 9 in Organization and Culture category. A full three stars in Throughput, Feedback and Payback. But is perceived to be very difficult. This assessment is probably due to the hero factor which is exhibited at UR. There is also a bit of a low bus factor in the DevOps team, but our clear impression is the difficulty lays with the system complexity and architecture. Meaning it is hard to get an overview of the dependencies and how changes may affect the system.   But the same may also be said about the Delivery Pipeline self assessment. This card scored a 9 in Build category. Again a full three stars across the value gauges but is perceived to very difficult to implement. It is, most likely, also rooted in the system complexity, architecture and dependencies.   Your self assessment on Testable code scored an 8 in Architecture and Design category. Only a two in the Throughput but a full three in the other two value gauges, while still being perceived as difficult to implement. This is a bit harder to analyze without really looking into the automated tests and the code itself.   Looking at the three blockers that stand out. We could imagine, that we see a red thread leading back to the system complexity, architecture and dependencies. What do you think?   Outliers   There was one self assessment that we wanted to highlight which did not fit into either low hanging fruits or blockers.      Use distributed VCS   Use distributed VCS scored a 6 in the Version Control category. You scored it a three for Throughput, a two for Feedback and a one for Payback. You perceived it as being difficult to implement. The result is that it does not end as either a low hanging fruit nor a blocker. However, we feel, you will find it difficult to implement an effective branching strategy that will help you in achieving a pristine integration branch in a CI/CD environment without moving to a DVCS. We think the Payback may be higher than a one.   Our picks for you   If we look at where our focus has been, and where you have self identified the most findings with high priority it falls in the following 7 cards, minus the Explicit knowledge transfer.      Buy-in from management   Pristine integration branch   Release train branching strategy   Code metrics   Dependencies are managed   Delivery Pipeline   Testable code   Your own picks regarding a release train branching strategy, and a pristine integration branch really fit well with where we could see you putting your focus to start. With the addition of:      Use a distributed VCS   Dependencies are managed and a Delivery pipeline also fit well with what we would recommend starting with. Mainly because we think it will give a good starting point for eating the elephant as we like to call it. We would also like to point out that the move to a distributed VCS could also help in this regard. It could give you the opportunity to discover implicit dependencies. See our solution Split them with an automated approach under the finding regarding Current VCS.   We left out the Explicit knowledge transfer because we feel it is a very difficult blocker to deal with presently. But also because we think that it will be less difficult as you make the, planned, incremental improvements. As you eat the elephant, institute a release train branching strategy, etc, there will be less of a need for hero’s.  ","categories": [],
        "tags": [],
        "url": "/customer_details/sample/report-parts/score/",
        "teaser": null
      },{
        "title": "Summary",
        "excerpt":" Note: the following is a sample summary from an anonymous client   In general we were quite impressed with your enthusiasm and commitment at Universal Robots. Such level of engagement is a rare sight, and it’s obvious to us that you take pride in your craft.   We found that you have a very high level of technical proficiency and continuously challenge each other, but do so in a very respectful and professional manner. Although it’s quite a challenge running so many workshops in such a short time span, this made running them with you very enjoyable and fruitful.   Some of the things you showcased during the workshops were also rather impressive. The fact that you have implemented quite a battery of automation and improvements as part of everyday work, also shows us that this is something you care for.   In the next chapter, we will go into detail on every observation we’ve made and noted. As not all of you will want to read the entire report cover to cover, this summary is presented as a narrative compilation of recommendations that scored a clean Must Have.   Culture and Organization   Some of your highly engaged teams have already identified many improvements and areas of technical debt. You’ve already got the mindset for the task and had already identified many of the common issues we usually encounter during these workshops. “It’s like an aircraft engine. You need to do the maintenance to keep it in the air” was an apt analogy you brought up. This is a great start, you understand the need to continuously take care of your asset.   However, your current approach was also described as an ad-hoc “boy scout” approach. Meaning you do the right thing by making improvements as you identify them. It creates hidden effort and can actually create more debt than it fixes. The approach to maintaining your asset needs to go to the shop, it needs to be analyzed and planned. Create them as issues in Jira, theme them, commit to doing them as part of your sprints and measure your progress. You have technical leads for the main subsystems. They should be driving this in a planned and organized manner.   There is also a certain level of acceptance towards manual testing from a cultural perspective. This means that tacking quality on is, to a degree, accepted. We think you should start with promoting testing to a first class citizen in the development process. There will always be some form of manual validation, but you want to ingrain testing as an automated process into your culture, as manual testing is inefficient. You can only do this if you implement some form of test management and define a strategy that will enable effective, flexible and scalable tests. This test strategy needs to be embedded into your culture and organization.   Code reviews should not be used for verification and should only be done after all automated verification has run. The best code reviews are those that check code semantics, often involving people with specific domain knowledge. Consider if pair programming is an option, which can be easily planned up front and helps creating cross-functional teams.   The last thing we’d like to highlight is the existence of team subcultures. Some of it may be domain pride, which is quite okay, but it seems to be partially driven misaligned processes across the teams, causing friction. Your branching strategies are a good example of this. How you develop software affects how you interact with each other. Aligning your important processes, like branching, versioning and test approach will give you a lot of value. At the end of the day you are delivering one product to the customer.   Tests   First and foremost, you need to get your tests under control. As it stands, they are really slowing you down and causing unwanted side effects such as long feedback loops, unstable pipelines, etc. The result is that the value of your pipelines are degrading, eventually leading to red builds being ignored.   We suggest you start by stabilizing the automated tests you already have. Valuable tests need to be fixed, while redundant tests need to be removed.   Focus on defining and implementing a test strategy and building up an inventory of tests. Follow best practice as far as the test pyramid goes. Rotating your pyramid 90° clockwise will give you a solid idea of when to run which tests in your pipeline.   Currently, your functional tests, also referred to as E2E, are being executed prior to the aggregated UR upgrade package. This is what you actually want to run such a high level of tests on. As functional tests are inherently more complex, they also affect the feedback loop, so run them after the first level of feedback is given to developers. In addition, consider running the tests in parallel where possible.   Version Control System   You need to move to Git. Subversion is simply too bulky and inflexible for a real CI/CD approach. It doesn’t do branching and merging very well, which you will be doing a lot in an automated release train workflow.   This is one thing we really feel you should focus on. You need to start defining your migration strategy.   You also need a defined branching strategy that supports a CI/CD and release train approach. This branching strategy should be the same across all the teams. This will enable automation and CI/CD by doing automated verification as a toll gate for entering the integration (main) branch. You want logical commits, which are automatically verified on branches and automatically merged into the integration branch.   Architecture   Large monolithic code bases, which take a long time to build and validate do not play well with CI/CD, regardless of the VCS or branching strategy chosen. A lot of the perceived problems are rooted in this: the complex build system, implicit dependencies, slow builds, etc. This is by far your biggest and most daunting task, but it is the one that will give the most value. We’re pretty sure we are preaching to the choir here. You want a more modular, loosely architecture.   PolyScope has some monolithic tendencies. Notable, there’s a single JAR that’s composed of 80% of the PolyScope code. You should investigate a more modular approach.   There is a tight coupling between PolyScope and the Controller. This can be loosened by splitting the dependency out into its own versioned and controlled artifact. An added benefit is that you’ll be able to manage the public API explicitly.   Breaking down your architecture into modules and creating individual pipelines for them implies you’ll need a structured and cohesive approach to dependency management and releases. Though we sadly didn’t have the time to dig deeply into this area, we know you are aggregating your dependencies in the firmware subsystem and the UR upgrade package. This means that you are already doing dependency management, you just need to make sure it is a cohesive approach where you can automate the creation of a versioned release from a branch and identify the dependency from the version. This is what the version, release and branching strategies will give you. The ability to automate the process.   Builds   One of your biggest pains right now is your unstable integration branch (trunk). The main drivers behind this are the lack of a cohesive branching strategy, unreliable tests and the absence of an automated toll gate. Start by setting up a toll gate to your integration branch. Define what makes code good enough to share with colleagues, e.g. can compile and run unit tests, and set this up as a barrier into the mainline.  Once development is done solely on topic branches and the automated toll gate is the only way into the mainline, you will have a stable trunk.   The build script looks to be a point of friction as it’s an attempt to combine some of the subsystem builds due to dependencies. The shell script is a pre-step, followed by Maven and SCons to build the subsystems. This is in reality just a reaction to the coupling between PolyScope and the Controller. The result is a slow feedback loop. This pre-step should be handled as a dependency instead of a build requirement, so you won’t have to build needlessly.   At least once, during the workshop, it was stated that not all of the build setup was found in scripts, but it was located somewhere in Jenkins. We have extrapolated this to mean the pipeline definitions. Your pipeline definitions are separate from the code. As a result, it is difficult for developers to get an overview of what they need to control. You need to build a better separation of concerns here. The DevOps team should provide the pipeline framework while the domain specific configuration should live in the repositories. The developers should be able to control what to build, which suite of tests to execute, etc. while the framework should just ensure that the definition of done is executed.   You need to push your pipeline into Front Office. The tests that are being done, or triggered manually, on the UR upgrade package should be part of a pipeline and the final image given to the customer needs to be generated as well. In reality your final automated tests should be done on the generated image. In general there are too many manual steps in the Front End release process. You should focus on automating these to the extent possible.   DevOps   It is quite evident that this has been an area of focus, a lot of work has been put into it and quite a lot of benefits have already been reaped. However, you have a low bus factor here. Therefore we would recommend that some effort be put into opening up the information bottleneck. DevOps, obviously, wants and needs a certain amount of control. But there is a fine balance between control and flexibility.   There are a couple of findings that have to do with resource management, both hardware and human resources. We recommend you take a closer look at how you manage both of these. Ask yourself what the best strategy for scalability from a hardware perspective is. Is it really fewer, more powerful, machines, or does the problem lay somewhere else? The one Windows build machine is an obvious bottleneck but you should make sure you identify the core problem.   Should the DevOps team really be maintaining the Jira and Confluence instances? We recommend that you shift such tasks away from the DevOps team and have them focus on feedback times, pipeline structure, automation, access to more production like environments, etc. The DevOps team should still own the ‘how’, i.e. the process flows, but shouldn’t have to deal with the installations.   Get the last pieces in place as far as your present infrastructure is concerned. You’re utilizing Docker images, which is great. Many organizations are far from containerization. But the base images are being built constantly. This means that they could move under your feet. Consider bringing Docker Registry into the picture.   Your environments should be managed as code. Start to make this a reality with a CMS like Ansible, Chef, Puppet, or whichever you choose. Use these clean environments as much as possible. Why could they not be the basis for developers environments?   A high level road map   For a high level road map of our recommendations, we’d suggest the following:      Move to Git and start using a branching strategy that supports a release train workflow.   Define and implement your toll gate into the integration branch.   Stabilize your integration branch and react when it goes unstable.   Define and implement your test strategy.   Start chewing away at that elephant! Break the monolith down with an iterative approach. It’s too big to eat in one sitting.   Automate Front Office’s manual processes and put them into a pipeline.  ","categories": [],
        "tags": [],
        "url": "/customer_details/sample/report-parts/summary/",
        "teaser": null
      },{
        "title": "Table of Contents",
        "excerpt":"  ","categories": [],
        "tags": [],
        "url": "/customer_details/sample/report-parts/toc/",
        "teaser": null
      },{
        "title": "Score",
        "excerpt":" ","categories": [],
        "tags": [],
        "url": "/customer_details/sample/score/",
        "teaser": null
      },{
        "title": "Artifacts Are Managed",
        "excerpt":" Note: The following is a curated mix of illustrative findings from various anonymized reports   Artifact management system configuration not optimal   The setup used in Artifact management is not as the tool is designed and intended for. The intended root structure is projects and on top of that a semantic structure with package and semantic version below. The current setup abuses the root structure as it is both the project and version which means that newcomers are confused and the root structure is polluted and grows as more releases are produced. The structure you’ve set up requires advanced logic in your build scripts rather than just simple SemVer logic.   Implement best practice structure   Store the artifacts in a hierarchical tree structure. It should also be done for non-Java artifacts if there is no explicit reason to not do so. Always respect the intent of a tool. Read the manual again, follow some of the tutorials for the tool. It will make the automation of the dependency management less complex if there is a consistent structure for addressing the artifacts.  ","categories": [],
        "tags": [],
        "url": "/customer_details/sample/snippets/build/artifacts-are-managed/",
        "teaser": null
      },{
        "title": "Automated Builds",
        "excerpt":" Note: The following is a curated mix of illustrative findings from various anonymized reports   It builds on my machine   Builds and tests run on individual developer’s self-maintained machines and workspaces, as no standard image for this exists. This results in an undesirable “Well, it works on my machine” scenario. One developer can have a hard time redoing another developer’s work. The team suffers from a lot of false negatives. Builds fail often. Not because the code is broken, but because the build environment is.   Use a clean standardized image for the toll gate job   Code changes should be built and tested on a standard and clean image. The image should be maintained by the team. This prevents local configuration from tampering with the builds, making them deterministic and consistent.   Developers use reference image   Ideally, there should be no difference between a developer’s environment and that of the toll gate. This is achieved by making it easy for developers to spin up the standard image to run their builds, allowing them to easily produce the same result as the toll gate job.   Final delivery to customer is not automated   The image given to the customer is generated before final test in a non-reproducible way. It is built manually based on an already running device with an other, older image. It is modified by manipulating files within the device before a final image snapshot. Conceptually the image could be broken during the creation process.   Automate image creation   Any release artifact should always be possible to reproduce without any doubt. It should be scripted 100%. Programmable and immutable.   Make the image generation stateless   A product to a customer should always be generated from scratch in order not to propagate error prone states. It should also always be reproducible through a declarative pipeline.   Automatically test the image   The automatically generated image is tested as part of a pipeline. You want to extend the pipeline as far down the release process as possible.   Release process is, to a large extent, manual   The majority of the release process located in the Front Office is a manual one, which is costly, lengthy and error prone, e.g. the image creation, release notes, support site updates, etc.   Design the release process for automation   It is important that each step is designed for automation. Make sure the physical resource are available and enabled for automatic deployment. It relates to servers, but also devices that they can be claimed, started and the execution evaluated.   Script each single step to enable automatic execution   Identify the process steps that are manual, entirely or even partially so. Automate step by step and integrate them into the pipeline.   Build script is an plaster on the monolith   The build script is created to handle the monolith by plastering each of the subsystem builds into a single step. The different subsystems have different build technologies (e.g. Maven and Scons), but in order to make it all work as a monolith it is wrapped with shell script.   Split the monolith and add dependency management of the artifacts   Splitting the monolith into subsystems and implementing true dependency management makes the build script obsolete. It simply vanishes. Please refer to the architecture cards for more information.   Localization files are introduced late in the release process, leading to unexpected UI changes   Localized text in the UI can cause the UI controls to change in some unexpected manner, e.g. resizing of buttons due to translated texts being longer than their original. Commit-stop is the point where no further features, and thus no new text, gets added. Translation can take up to one month and arrives fairly late in the release process.   Create requirements for localization subcontractor   Create criteria or a framework that helps these text translators adhere to font type, font size, etc. This gives them the opportunity to react early and discuss with you any problematic translations.   This will minimize, or even completely avoid, nasty surprises near the end of the release process.   Manual process to create a buildable workspace   It is not possible to start building from a clean checkout. You have to checkout two repositories and create a symlink in order to be able to build the software. It is a hidden dependency.   Make the ‘link’ permanent by using a explicit dependency   Create a ‘thin shell’ repository which contains the dependency between these repositories. In SVN it is called an external, in Git it is called submodules. You could also pull the dependencies from Artifact Management system if the repositories have their own pipeline and definition of done. The implementation depends on the technology. The important point is the dependencies are managed and versioned.   The pipeline scripts contain domain specific logic   Your pipeline scripts are bloated with domain specific logic and setup. Much of the script relates to setting up environments and requirements to build the software. The pipeline scripts become large and unmanageable.   Keep domain logic out of pipeline scripts   Consider your pipeline definition as configuration rather than a script to help keep it lean. Avoid building logic into the pipeline script that has nothing to do with either CI system’s interface, execution or reporting. Remember that all the logic you put in these scripts is also something developers would need to run at their desk, should they want to recreate runs locally. The pipeline script makes for a poor place to document these setup steps  ","categories": [],
        "tags": [],
        "url": "/customer_details/sample/snippets/build/automated-builds/",
        "teaser": null
      },{
        "title": "Automated Release Notes",
        "excerpt":" Note: The following is a curated mix of illustrative findings from various anonymized reports   Release Notes are manually created   The release notes are handled in Confluence. It is a manual process and it is time consuming. The Jira issues are extracted and post-processed for the final release note. It is glue-on quality - not built-in quality. The release note cannot be generated based on a meta-system like Jira alone. It does not have the source revision time record. Jira issues and releases in Jira do not understand relationship of changes in the SCM system.   Design your Jira issues for data extraction   It is important to configure you schema in Jira, so it stores all needed information about the logical change. Add a status which tells if it should be listed in the release note. Not every task should be listed in a release note that the end user sees. Add a field for end-user description which can be extracted if public.   Generate different release note for different stakeholders   The different stakeholders will have different needs. At least it makes sense to have two generated release notes. One for R&amp;D which is used for meta data quality checks and used to monitor ‘release-readiness’. The second is a public release note which is reporting what external stakeholders need to know. Meaning, which features have been implemented and which publicly known errors have been corrected.   Create your release note as a record of your SCM system   The only correct place to initialize your release note is from your SCM system as it is ‘the’ one and only truth about this particular revision compared to previous revision/releases.   Use a release note generator based on SCM system (M)   Given that the systems and process are designed for automation the report can be generated. Any changes to data for the release note is changed/added/removed at the root source of the information. This means that the release note can be regenerate based on the updated data.   Add the generator to the pipeline   Add the release note generation to the pipeline. It then becomes artifact as any other product generated as part of the pipeline. It will be an important artifact for the tester and product owner as it shows exact progress of the software for later promotion decisions on this particular revision.   Use the R&amp;D release note as a consistency check (S)   As part of the release gate it is possible to monitor the release readiness where the release DoD is programmed/implemented. This is to harvest traceability and use it as a consistency check.   The consistency checks could look something like the following      Do all SCM change-sets have a related Jira issue   Are all the Jira issues in a correct status   Do we have all the changes related to all the Jira issues for this release   Do the change sets, based on the Jira issues, have the needed reviews in the correct status   Security review process and documentation is manual   The security review is needed for the safety reaction of the device. Changes to these algorithms are related to a special and mandatory review that needs to be documented and signed-off as part of the release process. The review is not tool-aided and is currently documented in the Jira issue. It is a manual process to gather the reviews to create the document that is signed by the security-officer.   Implement a solid review process and automatically generate the documentation   It is advised to implement Crucible/FishEye(already available) as the front-end for the review. It handles the statuses of the reviews. Given the data and status is available in the Atlassian suite it is possible to auto generate the review report ready for signature.     An option is to use PAC as a foundation, it’s an open source tool that can be extended to also generate the review report.  ","categories": [],
        "tags": [],
        "url": "/customer_details/sample/snippets/build/automated-release-notes/",
        "teaser": null
      },{
        "title": "Feedback",
        "excerpt":"Continuous attention to your surroundings will provide you with the means for you to reflect upon your current situation. Think - and then improve it.   Feedback can be in the form of logs, data, graphs, statistics, analysis, discussions with customers or colleagues - Anything that gives your surroundings a voice.   If your Continuous Delivery ecosystem or your application isn’t talking to you, then you’re trying to move forward while blindfolded, in the dark.   Give your surroundings a voice.   For each card ask yourself the following question:      To what extent would implementing this feature or ability provide more feedback or visibility to the responsible worker and his colleagues?    The feedback is measured from one to three stars. Use the stars as follows:         It sporadically affects the developer at an individual level.         It affects the entire team and collaboration.         It affects us, beyond the project, at a corporate level.  ","categories": [],
        "tags": [],
        "url": "/gauges/feedback/",
        "teaser": null
      },{
        "title": "Payback",
        "excerpt":"For most software projects it’s the code’s complexity and technical debt that eventually makes it obsolete.   Every time we touch the code, we’re jeopardizing the original design and quality. Either by adding quick solutions that don’t scale or because we simply don’t add non-functional ‘abilities’ to the solution.   Testability, traceability, maintainability, supportability, readability… These kinds of abilities.   We’re building up technical debt - debt that will have to be paid back or you’ll end up broke.   For each card ask yourself the following question:      To what extent would implementing this feature or ability help to pay back on technical debt or actively prevent more debt from being added?    The payback is measured from one to three stars. Use the stars as follows:         It sporadically affects code snippets at an individual level.         It affects design at a team collaboration level.         It affects architecture at a corporate asset level.  ","categories": [],
        "tags": [],
        "url": "/gauges/payback/",
        "teaser": null
      },{
        "title": "Score",
        "excerpt":"The card score is calculated based on the values of the gauges.   The algorithm used is:   (throughput + feedback + payback)   Each card is also assessed for complexity, you should pay attention to the cost of implementation:    It’s a low hanging fruit.    It’s as a team effort.    It’s a mountain to climb.  ","categories": [],
        "tags": [],
        "url": "/gauges/score/",
        "teaser": null
      },{
        "title": "Complexity",
        "excerpt":"Complexity is everywhere.   In a standard approach to software development, complexity is the meter that influences all others.   The simpler a solution is to implement, the faster the Return On Investment.   For each card ask yourself the following question:      What would be required to implement this feature or ability, taking into account man hours, money spent up front, and ongoing cost of ownership?         The first level of complexity is something an individual can do within project scope. It’s marked as a low hanging fruit .        The second level of complexity is something that can be achieved as a team effort, still within a project scope. It’s marked as a team effort .        The third and highest level of complexity is something than can only be achieved with permission and funding from the line-organization. It’s too big for the project to handle within it’s own means and resources. It’s marked as a mountain to climb  .  ","categories": [],
        "tags": [],
        "url": "/gauges/simplicity/",
        "teaser": null
      },{
        "title": "Throughput",
        "excerpt":"An interesting measure to focus on is simply speed or throughput. The time it takes from a developer proudly shouting “It works on my computer!” to it actually working on the end user’s computer.   For each card ask yourself the following question:      To what extent would implementing this feature or ability improve the overall throughput of my pipeline?    The feedback is measured from one to three stars. Use the stars as follows:         The improvement can be counted in minutes.         The improvement can be counted in hours.         The improvement can be counted in days.  ","categories": [],
        "tags": [],
        "url": "/gauges/throughput/",
        "teaser": null
      },{
        "title": "Advanced",
        "excerpt":" ","categories": [],
        "tags": [],
        "url": "/levels/advanced/",
        "teaser": "/assets/images/cards/codememo.png"
      },{
        "title": "Beginner",
        "excerpt":" ","categories": [],
        "tags": [],
        "url": "/levels/beginner/",
        "teaser": "/assets/images/cards/codememo.png"
      },{
        "title": "Expert",
        "excerpt":" ","categories": [],
        "tags": [],
        "url": "/levels/expert/",
        "teaser": "/assets/images/cards/codememo.png"
      },{
        "title": "Intermediate",
        "excerpt":" ","categories": [],
        "tags": [],
        "url": "/levels/intermediate/",
        "teaser": "/assets/images/cards/codememo.png"
      },{
        "title": "Novice",
        "excerpt":" ","categories": [],
        "tags": [],
        "url": "/levels/novice/",
        "teaser": "/assets/images/cards/codememo.png"
      },{
        "title": "About the Continuous Delivery Maturity Model",
        "excerpt":"This model originates from the time when I founded and operated Praqma. A company specialized in Continuous Delivery implementation of software development processes. Praqma is now gone. The company was swallowed in an merger and acquisition process in 2019 and the model didn’t survive. It was stored away in a GitHub repository and never used again.   …until now.   History   Praqma was, in it’s early days, focusing on configuration management, version control systems, build optimization, build automation, static code analysis, test automation. Around that time we would probably refer to all that as just “CI - Continuous Integration”. In Praqma we were very engaged in the Jenkins CI community. We had developed more than 20 plugins as commission work for our clients and released them as Open Source to the community. We had a very close collaboration with CloudBees - the company behind Jenkins CI and Kohsuke Kawaguchi who originally developed Hudson (the predecessor to Jenkins) as an internal project at Sunn, and at that time he was the Jenkins CI Open Source Community lead - on CloudBees’ pay-roll.   Version 1 - CI focus    I presented the first version of the model at the first Scandinavian Jenkins User Event in 2013. It was part of our dedicated focus to shift from being a CI company to a “CD - Continuous Delivery” company. At that debut presentation it was nothing more than a poster:   CloudBees still has a blog post online: “What is Continuous Integration?” that includes a picture of this first version.   Base on this version we developed a simple survey in a google form, and a few scripts to analyze the data. Our goal was to map the current state - and interest - in Continuous Delivery in the industry. The survey picked up more than 400 replies over the course of a just a few months.   Based on these replies and then general feedback on the survey we realized that we needed to upgrade the model - and explain it.   Version 2 - a model framework   In December 2015 we released the next generation of our model. It was not only a new version of the content in the model but an entire framework build in MarkDown and Liquid to enable us to easily update the model, based on feedback, and even adapt it to specific contexts or companies.   The model was optimized specifically to be used as an assessment tool for helping companies to map their current Value streams and to build a backlog for implementing Continuous Delivery. Our assessment process was based on “The Toyota Way” principles: Value Stream Mapping, Genchi Genbutsu (observe deeply), identifying waste (muda, mura and muri), measure lead times and Continuous Improvement (kaizen) based on 5S (sort, straighten, shine, standardize, sustain).   We wanted to create a unique assessment process where it was the customer’s own employees - the contributors to the value stream - that delivered the insights, we merely facilitated the process and curated and evaluated the findings.   The assessment process we developed would span five intense workdays on-site at the customer, with all hands on deck. Some of the larger assessments we did had 120 participants in the first-day Value Stream Mapping workshop.   We gameified the process with huge rich pictures on the wall and we developed a deck of cards - one for each tile in the matrix - which we used in different workshop formats where the participants would score the cards by four different gauges and hereby identify both “low hanging fruits” as well as “blockers”.   The model framework would build a static website, but it had a unique feature that the .pdf to print the card deck from was automatically generated on-the-fly, so any last-minute changes, adaptions or customizations to the model could be implemented in matter op minutes.      The model framework, was later extended so that event the findings we observed during the assessment - the Continuous Delivery backlog - and the report to the customer including the executive summary - was written directly into a customer specific version of the model. The result of the process was delivered to the customer both as a static website that they could read in a browser and to accompany it a 50-70 page detailed report in .pdf. A report that the model framework automatically created based on the website content.   We had made the model a living example and a show case of what we was preaching - continuous delivery, anything-as-code, in this case even the customized model, the card deck, the result of the assessment.   Practical - real life use   From late 2015 and until I left Praqma in 2018 I lead the development of this framework and Continuous Delivery assessment format and in that period I conducted 20 assessment. And we even managed to standardize the format and process to a degree where my colleagues at Praqma was able to execute the same assessment four times, without my attendance.   Praqma’s last official version of the model is captured by the Way Back Machine: code-maturity.praqma.com   A live sample - it’s just a story   Here on there pages a lakruzz.com I’m now re-publishing the model. It has received a large overhaul. Under the hood a large majority of the code is rewritten, to enhance readability in general and media queries and responsive design on handheld devices. But more importantly I’ve turned the model into a template you can inherit from in your own static website in jekyll - I’ll give a few pointers to how that can be done later. but for now I just want the release this MVP so we can get the model live again.   To show-case the features of the model I have added a anonymized and generalized write-up of some of the most typical or illustrative findings and mitigation from the total of 24 assessments that was conducted on the basis of this specific model. I started out with the three first cards in the Build area - more will follow. You can subscribe to the feed in the footer to get updates.   Any references to any the 24 companies or even the specific mentioning of their tool-stack, organization, location, domain of operation, or any data for that matter, that could possibly identify them is weeded out or scrambled.   So what is left on these pages is an example or a story of what a Continuous Delivery Assessment report based on this model could look like.   But hey! Feel free to read much more into it than just that; I can reveal to you, that a large majority of the most important findings in the various areas and levels were common and shared among many of the companies. In short; their issues and challenges were seldom unique and even if this sample assessment is not based on observations done in your specific organization I would be surprised if your weren’t able - none the less - to read some of it, as-if it was your own assessment.   How to read the model      Hit the icon in the top-left corner to go to the front of the model. You will see 5 levels of expertise - Novice, Beginner, Intermediate, Advanced and Expert. These levels are a chosen as a reference to The Dreyfus Brother’s model for skill acquisition which contributed to the field of Cognitive Science as early as in the 80’s to propose an early model for understanding intelligence.   In context of Continuous Delivery we chose to focus on six different areas of knowledge. Build, Test, Version control, DevOps, Architecture &amp; Design and Organization &amp; Culture. These categories can be seen as a-sign-o-the-times - back then. You would think that some of these areas no longer impose a problem. Surely for use on modern software development products and teams these areas should be updated.   Maybe, sure! - But then again; you would be surprised how slow the world is moving - in large organizations.   I did make one specific change;  In the Version Control area I moved the card “Use Distributed VCS”  from the Advanced level to Novice. In the eight years that has passed git has become so much af a standard that it’s unavoidable - using git is now considered basic stuff.   Each tile in the matrix has it’s own landing-page which contains:      A score on each of the four gauges Throughput, Feedback, Payback and Complexity and a total score, which is the sum of the effect; Throughput + Feedback + Payback.   A short description.   A list of findings marked with .   Each finding is then followed up by least one, sometimes more, suggested possible mitigation marked by prioritization; High , Medium  and Low    What to look for: As mentioned earlier, the gauges, the total score and the level of complexity help to easily identify:   Low hanging fruits   Findings that score high: in the range 7-9. and marked as 1st level of complexity with  are low hanging fruit. These are issues that would have a huge impact and that are relatively easy or simple to implement.   Team efforts   Findings that score high: in the range 7-9. and marked as 2nd level of complexity with  are team efforts. They are probably more important than some of your low-scoring low-hanging fruits, but easy to overlook, because they are not necessarily easy.   Mountains to climb   Findings that score relatively high in in the range 7-9 and marked as 3rd level of complexity with  are Mountains to climb — or even blockers. They can be equally important to fix as the low hanging fruits but due to the nature of these; they are difficult and often expensive, maybe even impossible to fix. So teams and organizations tend to ignore them.  ","categories": [],
        "tags": [],
        "url": "/about-model/",
        "teaser": "/assets/images/cards/codememo.png"
      },{
        "title": "Admin",
        "excerpt":" Printables      Printable cards   All card backs for reading   Full Report            Table og Contents       Executive Summary       Context       Findings       Scores                    Analysis                       Tools          ","categories": [],
        "tags": [],
        "url": "/admin/",
        "teaser": "/assets/images/cards/codememo.png"
      },{
        "title": "I'd ❤️ to chat with you",
        "excerpt":"Send an email to the Editor in Chief   Get perspective on a tech challenge   Let’s grab a coffee or a lunch - In real life - Away From Keyboard.   This is like a free coaching session - we will talk about you and your challenges - not mine. I offer that (professional) shoulder you can cry on. Let go! I’ll listen and support you, and offer perspective.   Book a meeting   Get a free workshop for your team   We have a series of workshop and speaks that I have prepared, and done publicly and for free. I’ll be happy to do it again. Despite that they are free in the literal meaning “costless” they are not cheap in the meaning “inferior” or “low quality”. Quite the opposite, they showcase the seriousness and high quality I aim for in my engagement.   I offer them as a private V.I.P. session. Only condition is that you sign up min 6 people (no max!) and that you set aside time to both prepare well before the workshop, and evaluate with me after.   Book a meeting   Give feedback on the website   Ohhh - thanks :pray: I’m crazy about feedback.   I have turned on public comments on the pages using GitHub. You will need an account on GitHub to use this option - create one for free.   Reach out to the Editor in Chief      Would you like me to cover a specific topic?   Do you have questions regarding any of the content on this web?   Do you have wishes for a workshop or hackaton I could host?   Anything….   Send an email to the Editor in Chief  ","categories": [],
        "tags": [],
        "url": "/contact/",
        "teaser": "/assets/images/cards/codememo.png"
      },{
        "title": "Gauges",
        "excerpt":"                                                                               Throughput                                                                The speed of the pipeline                          An interesting measure to focus on is simply speed or throughput. The time it takes from a developer proudly shouting “It works on my computer!” to it actually working on the end user’s computer.   For each card ask yourself the following question:      To what extent would implementing this feature or ability improve the overall throughput of my pipeline?    The feedback is measured from one to three stars. Use the stars as follows:         The improvement can be counted in minutes.         The improvement can be counted in hours.         The improvement can be counted in days.                                                                                                        Feedback                                                                Reflection in action needs a voice                          Continuous attention to your surroundings will provide you with the means for you to reflect upon your current situation. Think - and then improve it.   Feedback can be in the form of logs, data, graphs, statistics, analysis, discussions with customers or colleagues - Anything that gives your surroundings a voice.   If your Continuous Delivery ecosystem or your application isn’t talking to you, then you’re trying to move forward while blindfolded, in the dark.   Give your surroundings a voice.   For each card ask yourself the following question:      To what extent would implementing this feature or ability provide more feedback or visibility to the responsible worker and his colleagues?    The feedback is measured from one to three stars. Use the stars as follows:         It sporadically affects the developer at an individual level.         It affects the entire team and collaboration.         It affects us, beyond the project, at a corporate level.                                                                                                        Payback                                                                The payment to your technical debt                          For most software projects it’s the code’s complexity and technical debt that eventually makes it obsolete.   Every time we touch the code, we’re jeopardizing the original design and quality. Either by adding quick solutions that don’t scale or because we simply don’t add non-functional ‘abilities’ to the solution.   Testability, traceability, maintainability, supportability, readability… These kinds of abilities.   We’re building up technical debt - debt that will have to be paid back or you’ll end up broke.   For each card ask yourself the following question:      To what extent would implementing this feature or ability help to pay back on technical debt or actively prevent more debt from being added?    The payback is measured from one to three stars. Use the stars as follows:         It sporadically affects code snippets at an individual level.         It affects design at a team collaboration level.         It affects architecture at a corporate asset level.                                                                                                        Complexity                                                                The simpler it is to implement the higher the gain                          Complexity is everywhere.   In a standard approach to software development, complexity is the meter that influences all others.   The simpler a solution is to implement, the faster the Return On Investment.   For each card ask yourself the following question:      What would be required to implement this feature or ability, taking into account man hours, money spent up front, and ongoing cost of ownership?         The first level of complexity is something an individual can do within project scope. It’s marked as a low hanging fruit .        The second level of complexity is something that can be achieved as a team effort, still within a project scope. It’s marked as a team effort .        The third and highest level of complexity is something than can only be achieved with permission and funding from the line-organization. It’s too big for the project to handle within it’s own means and resources. It’s marked as a mountain to climb  .                                                                                                        Score                                                                Adding priority to the cards                          The card score is calculated based on the values of the gauges.   The algorithm used is:   (throughput + feedback + payback)   Each card is also assessed for complexity, you should pay attention to the cost of implementation:    It’s a low hanging fruit.    It’s as a team effort.    It’s a mountain to climb.                                        ","categories": [],
        "tags": [],
        "url": "/gauges/",
        "teaser": "/assets/images/cards/codememo.png"
      },{
        "title": "Score",
        "excerpt":"                                                                                                                                               Commits are tied to tasks                                                               0                                                 Don't change the code for no reason at all           Every time the code changes, it traces back to some task being done. Think about it; Who’s feeding the process? How are tasks being created, prioritized and executed?   When a code change cannot be tied to a specific task, it’s likely something that has bypassed the normal chain of command. Somebody is working outside protocol.   Make all code changes part of an overall plan - simply pair commits with tasks. Besides generating valuable traces that can later be used in an audit or documentation of your trail, it also enables you to track the pace of the team and maintain a burn-down chart.                                                                                                                                                                                Pristine integration branch                                                               0                                                 No offense, but you couldn't break the build even if you tried           To integrate means to merge your code on to the same branch as the one your colleagues are working on.  So obviously if your code breaks something you are potentially jeopardizing the work space - and pace - of your team mates as well. To have a pristine integration branch means that it is buildable at all times.   Code should be verified through some kind of toll-gate criteria, before it’s accepted on to the integration branch. Anything that doesn’t meet the toll-gate criteria is rejected and will not enter the mainline. It is simply impossible for a developer to break the build.                                                                                                                                                                                Release train branching strategy                                                               0                                                 Tug along or be left behind - we're releasing on rails           The release train branching strategy is similar to what is sometimes referred to as late branching or trunk based development. Essentially it implies that in your entire branch tree there is only one branch that is meant to be long-lived.   Consequently, there is only one way you can contribute to a product and have your code released, and that is to deliver your code to the mainline.   In a release train strategy, the mainline is on it’s way to production all the time, and therefore anything that isn’t meant for production as soon as possible, shouldn’t be delivered to the mainline.                                                                                                                                                                                Use distributed VCS                                                               0                                                 Embrace the full power of modern versioning           Distributed version control systems are a faster, modern alternative with a healthy community. Due to its distributed nature, switching to git opens up many doors to automation. e.g.: It allows for automation tools to work in a local repository without jeopardizing the mainline.   Keep your project’s history clean and understandable. Make it easier to find specific commits and for others to review.  When finishing up work in your short-lived branches, clean up your local commit history before merging back into the integration branch.                                                                                                                                                                                Version numbers matter                                                               0                                                 Your software is more than just a number           Versioning schemes are a powerful tool.   They give you a quick and accurate reference to when, where and by whom something was made, what it’s compatible with, etc. It is the name of your release, the identifier of your component, the passport of your product.   Implement a well-defined versioning scheme for your components and releases.                                                                                                                                                                                Artifacts are managed                                                               6                                                 Are you building that over and over and over again?           An artifact is the output derived from your build process.   Sadly, artifacts are often built whenever they’re needed. A lot of the builds just build that which has been built before.   Even though this could be about contributing to avoiding the environmental crisis, it is also justifiable to save and manage artifacts simply to save wait-states and bottlenecks in the software development process.   Stop building things that haven’t changed and start reusing colleagues’ artifacts - install an artifact management system.                                                                &#xf0aa;&nbsp;Read more…                                                                                                                                                                    Automated builds                                                               7                                                 Yes. But it doesn't build on THAT machine.           When code changes are committed to the repository, your CI server kicks in automatically and starts a build.   It might not even be an actual build, it can be any kind of automated action that is part of a verification process implied in the project’s “definition of done”.   If a build step fails, the developer is notified directly so he can start working on fixing the issue immediately. The shorter the feedback loop, the better.                                                                &#xf0aa;&nbsp;Read more…                                                                                                                                                                    Automated release notes                                                               5                                                 If it's boring - automate it. Release notes are boring.           Whenever you ship a new release you probably need a release note, a report listing the new version number, fixed issues, new features…   Why write all that manually? By building up your traces and recording your trails as you work, you can pull all this information out of your backend automatically.   You’ve written your last release note. From here on out it’s release notes as code.                                                                &#xf0aa;&nbsp;Read more…                                                                                                                                                                    Delivery pipeline                                                               0                                                 A full ride produces a release candidate           Split up your builds and verifications into a pipeline consisting of multiple stages. Use this approach to keep your builds as fast as possible, your feedback loop as short as possible and your developers notified as quickly as possible despite having long-running builds.   In your pipeline, each step provides more confidence in your code than the previous one.                                                                                                                                                                                Full traceability                                                               0                                                 Run a tight ship - control software from cradle to grave           Take an arbitrary piece of hardware; As part of the Product Lifecycle Management the manufacturer has a complete trace to the individual versions of all the components that went into it.   Application Lifecycle Management means that you apply the same approach to software. You trace everything.   The requirements that created the tasks, the commits that resolved them, the compiler that built them, the tests you ran, the environment you ran them in, the test results and so on. When the software reaches production, you know the specifics of everything that created it.                                                                                                                                                                                Code metrics                                                               0                                                 Crunching the code will reveal trends and tendencies           Analyzing your code is cheap but valuable. Scouring your code and producing interesting metrics helps you keep check on all kinds of creeping issues.   Static code analysis, style checkers, cyclomatic complexity, code coverage and scanning for FIXMEs and TODOs are all examples of metrics that help you keep a watchful eye on codebase evolution. Adding thresholds to these measures as part of your verification protects it from corruption.   Monitor improvements but don’t waste time reaching arbitrary targets.                                                                                                                                                                                Dependencies are managed                                                               0                                                 Get your dependencies straight           All software has dependencies; You may be using third party technology or you have a lot of individually released microservices, frameworks or libraries in your system architecture.   Make sure there are no moving targets and don’t rely on someone else’s master, latest  or stable release. Cache everything you need in your own registry.   Optimize your link processes to use cached dependencies when available, optimize your compile processes to feed the registry when new versions are created, so others can benefit from them.                                                                                                                                                                                Full audit trail in production                                                               0                                                 Oooh - If only you knew what happened before the smoke came           When something goes south - it’s usually in the production environment - where you don’t have access to debug or profiling information.   Design you code, so it can produce an audit trail - a complete profile of states, sequences, data in and out. That should give you clues, when you try to do your code-scene forensics.   At least you’ll get some clues on how to reproduce the error in your development environment.                                                                                                                                                                                Individually releasable components                                                               0                                                 Components should have low coupling and be self-contained           This principle lends itself to common coding standards of high cohesion and low coupling.   Break down your monolith, identify all the nuts and bolts in your architecture that produce an actual artifact - like a binary executable from compilation or any other kind of package.   Make these components self-contained with it’s own definition of done, it’s own pipeline, it’s own interface - it’s own release process.   Treat it as inventory and manage your dependencies.                                                                                                                                                                                Testable code                                                               0                                                 Testable code gets tested - doh!           Whether or not a particular code snippet gets tested is often a matter of how easy it is to test.   Organize your code in easily accessible features. Make each feature available through one interface only.   Since the feature is only available through one interface, it’s safe to consider it tested, when you have massaged it.   At the end of the day, more code gets tested.                                                                                                                                                                                Automated deployment                                                               0                                                 Don't use manual processes for your most important step           The ability of a release to be deployed is such an essential part of the delivery that the developer is expected to take full responsibility for this process.   The deployment should be automated because it’s a task that needs to be carried out often and is not necessarily trivial.   You want your deployment as code.                                                                                                                                                                                Infrastructure as code                                                               0                                                 The best way to define, document and deliver your IT is with code           Every aspect of your entire development and release process can be traced back to some kind of version controlled code.   This can range from the versions of your dependencies to the configuration of your CI server pipeline.   In this context as code means that it’s persisted in files, it’s syntax can be checked, it has semantic meaning, it’s version controlled and can be executed.                                                                                                                                                                                Live monitoring and feedback                                                               0                                                 It's 10pm. Do you know where your code is?           Your software is in production, but how is it doing?  You want to have insight into the runtime health of your system.   This includes easy access to runtime statistics such as feature usage, transaction throughput and error situations to ensure the service level.  In addition, access to environment health like disk and memory usage, cpu load.   Bonus points if your system can alert you before an error occurs.                                                                                                                                                                                Access to production-like environments                                                               0                                                 Don't let infrastructure be the bottleneck           Both successful deployment and functional testing are often considered part of the definition of done.   When we´re building continuous delivery pipelines it’s because we want our developers to have access to, and to execute, the full definition of done.   Use simulators, emulators, containers and VMs to verify your definition of done and ensure stable releases. The closer to production your testing environment, the more confidence you can have in your changes.                                                                                                                                                                                One Team                                                               0                                                 No dev, no ops, no QA - just us           Groups of professionals contributing to the same product that are working in isolated silos and not talking to each other is not helping your project one bit.   Break down these silos by having contributors talk to each other and involving them in the bigger picture.   The term full-stack-developer is used to describe a developer whom is fully capable of doing whatever is required. This is the essence of not working in silos.                                                                                                                                                                                Agile process                                                               0                                                 Not Agilefall or Watergile - just plain agile           Agile processes defies phases in the software development process. The 1st principle in the Agile Manifesto refers to continuously delivering software.   An agile approach is not a small waterfall or a interactive and incremental process speeded up to 14 days intervals. It literally has no phases - only continuous integration and continuous delivery with focus on minimizing work in progress and get the right thing done - at a constant pace.                                                                                                                                                                                Buy-in from management                                                               0                                                 Continuous Delivery is not a grass root approach           Initiatives, new tools and approaches are often of natural interest to developers and they might experiment and research without explicitly being told to do so.   But Continuous Delivery is a paradigm that strives to build quality into the product rather than gluing it on afterwards.   Transitioning is going to take time. It needs planning. It needs prioritization. It needs funding. You need a road map. Continuous Delivery is not a quilted patchwork. Be sure to make it a corporate thing, not just a neat idea.                                                                                                                                                                                Designated roles                                                               0                                                 A designated driver brings everybody home safe           Shared responsibility often leads to misunderstandings. When the people involved  rely on the others to manage that responsibility.   Even if the responsibility is assigned to a role, and that role is given to one person only, it’s often the case that the person hasn’t allocated time to actually perform the duties.   Every process, that’s required, needs to be assigned to a role and that role needs to be assigned to a person, who is actually expected to responsibly spend time on performing these duties.                                                                                                                                                                                Explicit knowledge transfer                                                               0                                                 Aim for a high \"bus factor\"           The bus factor measures how many people in your corporation need to be run over by a bus before you go out of business. If you have a key player who’s indispensable, then your bus factor is 1.   To raise the bus factor, you must make sure that important knowledge is shared and accessible to whoever needs it.   Don’t document your processes to the brink of boredom or maintain an internal wiki the size of Wikipedia itself. Build a learning organization that encourages people to share with colleagues, allocates time for research, designs for change and accepts automation as documentation.                                                                                                                                                                                Tasks are groomed                                                               0                                                 Size matters - small is beautiful           Assignments should be prepared for working before they qualify as actual tasks. The goal of a task must be known to the person who is implementing it.   If a task is ambiguous it can not be estimated, and if it can not be estimated, it can not be prioritized.   If a task doesn’t have a clear definition of done, then it should be time-boxed.                                                                                                                                                                                Adaptive test suites                                                               0                                                 Only run the required tests - you know which ones I'm talking about! Right?           When your test cases are self-contained with individual setups and tear-downs and they trace to related functions and features, you are able to analyze a given change set, place it in context of a limited amount of features and derive its relevant test cases.   Then you can construct an adaptive test suite on the fly and execute that on a production-like environment.   By running a small and relevant subset of functional tests, you can add functional testing to the short feed-back loop.                                                                                                                                                                                Automated functional tests                                                               0                                                 Testing from start to finish, automagically           In a functional test, you test the features that the system offers as a whole, seen from the end user’s perspective.   In the previous millennium such a test would be planned by a person with domain knowledge, then executed prior to every release by testers performing manual operations based on written instructions.   A more contemporary strategy is to have the person with domain knowledge manage a team of developers, who are actually implementing the tests as code, and then give the software developers access to execute these test in their production-like environments.                                                                                                                                                                                Maintain test data                                                               0                                                 Test data as code           Management and maintenance of your test data is considered part of your Quality Assurance strategy. Your test data is versioned and stored as an artifact.   This implies that you separate your test data from the actual tests, which in turn comes with the benefit of easily running test suites with different, versioned data sets.   Test suites becomes self-contained, each with their own easy reproducible setup and tear-down steps, something that will later enable you to run your test suites independently of each other - maybe even selected on output from previous verification steps in your pipeline.                                                                                                                                                                                Test in production                                                               0                                                 Murphy and me - Errors will eventually happen in production           A word of precaution; testing in production is not to be confused with releasing untested code.   It starts with acknowledgement that all serious problems are discovered in production and occurred because unforeseen things happened.   Deliberately go to your production environment and do unforeseen things like turn off a server, kill a process, pour coffee on your keyboard, upgrade a service during high-load.   If your system is built to survive it, then it should! You’re only sure it will if you (dare) test it.                                                                                                                                                                                Unit testing, mocks, stubs and proxies                                                               0                                                 Keeping a check on your code's behavior           Unit tests are used to test the semantics of your code; To verify it works as expected and keeps working as expected through changes.   Unit tests are light-weight and fast. Don’t get tangled up in hard-to-handle dependencies such as loading databases or instantiating long sequences of objects before you get to the actual testing, use mocks and stubs to simulate your first order dependencies. Or use Proxies to have non-local collaborators contribute to your test.   A unit test is quick to execute and it should be executable in the context of your development environment.                                                                ","categories": [],
        "tags": [],
        "url": "/score/",
        "teaser": "/assets/images/cards/codememo.png"
      },{
        "title": "Continuous Delivery Stories",
        "excerpt":" ","categories": [],
        "tags": [],
        "url": "/stories/",
        "teaser": "/assets/images/cards/codememo.png"
      },{
        "title": "Terms and Privacy Policy",
        "excerpt":"Privacy Policy   This site is operated from an obligation and dedication to protect the confidentiality, integrity and accessibility of it’s user’s data, including personal data. I’m careful in on ensuring compliance with applicable data protection legislation, first and foremost the General Data Protection Regulation (GDPR).   Editor in Chief   The Editor in Chief is me - Lars Kruse. If you have any questions about the data policy presented here, or if you need to report content you’ve found on these pages; please do not hesitate to reach out to a me   lakruzz.com is not a business   This is a personally driven - non-profit - site. On these pages each author speaks with her or his own tone-of-voice. On these pages the editors are dedicated to sharing information and growing a community - not running a business. All material on theses pages is free. I deliberately use authors in plural, as I may host material from others too, but I remain the Editor in Chief. I take the responsibility.   The main purpose is to expose and facilitate my personal passion for utilizing technology and digital solutions to drive meaningful outcomes for people. I love to share knowledge and experience with others. Most people in tech do. Therefore the stories have interactive discussions enabled, and you are encouraged to join and set your mark - all it takes is a GitHub Account.   Data controller   This site processes data - consequently I consider myself a data processor.  Although not strictly required in relations between private individuals I take on this role in compliancy with he GDPR rule set.   My interpretation on maintaining this role is based on the EU commission’s adequacy decision which clarifies that for US companies that has signed the EU-US Data Privacy Framework program and which offers Standard Contractual Clauses (SCC) that have been “pre-approved” by the European Commission, the legal implication of utilizing services from US based companies is considered adequate to be fully comparable with that of obtaining similar services from EU based companies.   I carefully monitor, that I do not utilize services from US based companies unless they have signed the Data Privacy Framework program and offer SCCs in compliance with the EU Commission’s recommendations.   As an example: I have a paid account with GitHub. As part of this contract I’ve accepted GitHub’s Standard Contractual Clauses which are aligned with the EU commissions recommendations. GitHub has signed the Data Privacy Framework program therefore; When I host these page on GitHub pages it’s in compliance with the EU commission’s recommendation of data transfer to 3rd party countries outside EU.   Same logic follows for the Google Analytics tags I use, since Google also offer terms that are compliant with the EU SCCs and they too signed the Data Privacy Framework program.   The cookies we set      I set one cookie of my own, to track that you answered ‘yes’ to use the site and let us use cookies   I set two Google Analytics cookies, that helps me with insights into how the guests browse the website but I guarantee that I do not collect an Personally identifiable information (PII) about who you are.   The site does not collect any PII data   Personally identifiable information (PII) is a legal term used to define any data that can be used to identify someone. On this specific low-fidelity site all data I track is anonymized and I do not elicit any kind of PII data - not even you IP address.   Embedded pages, services and redirects to other services   Occasionally this site may use embedded pages (&lt;iframe&gt;) or include other services (&lt;script&gt;). For instance that is the case for the ability to leave comments on the pages. The comments are hosted by GitHub. And pages may also embed Google maps, or redirect to external sites, such as Meetup.com to signup to events.   On these external pages, embedded or redirected, you choose yourself, at your own will, wether you want to sign in or not. How you act and behave on these other pages and services - is considered outside the scope of this data policy you are reading here.   Also note, that on external pages, there might be functional cookies set. When that is the case, these are beyond the scope of the cookie consent you have given to this website.   If you find scripts running or cookies set in the scope of this website that you would like to question, then I’m happy top explain what’s going on - reach out to me: the Editor in Chief.  ","categories": [],
        "tags": [],
        "url": "/terms/",
        "teaser": "/assets/images/cards/codememo.png"
      },{
        "title": "The DORITH principle",
        "excerpt":"DORITH is a contraction of sentence “DO the RIght THing”. It’s a principle that enables you to make decisions that will appear to be tactical and still allow you to learn as you go. Understand the in-situ challenge with  being tactical and start taking small steps in the right direction.   Vision, strategy and tactics …yadda, yadda, yadda   It’s well know, that in management and leadership, the terms vision, strategy and tactics are as old as Methuselah. And all though the correlation between the terms and the way they are used does make a lot of sense - in management. It’s also one of those doctrines, that kinda make us immune, isn’t it? When a manager says “Vision, strategy and tactics” we tend to roll our eyes and think “yadda yadda yadda - talk to the hand”.   The in-situ problem with being tactical    I’ve been managing people, for ten years now, and from that point of view it really does seem to be important to ensure, that the everyday work is tactically founded in a strategy that leads to fulfillment of the vision. However, it only makes sense to think like that, it doesn’t make much sense to talk like that.   Here’s an example; When someone walks up to me and ask for advice or priority or mentoring in relation to something very specific that they are currently occupied with. Then it doesn’t make much sense to reply “be tactical about it” although that is technically not a bad reply at all. In reality however, it’s very rare that it resonates with something concrete. It’s usually quite abstract what tactical is in the given context.   Another problem with that technically correct, but in-situ fairly useless reply, is that it ties back to the strategy; Being tactical implies “…according to the strategy” and even worse - being strategic likewise implies “…according to the vision”.   Damn - that means, that as a manager, I need to have a rock solid and well communicated vision, that is manifested in an equally rock solid and well communicated strategy - The truth is, that in our company, these strategies are not manifested or rock solid, it’s more that we strive to build them into our corporate culture and that we’re allowing them to be somewhat in flux.   Without these strategies and employee handbooks, I can understand why the encouragement to be tactical leads to the immediate “yadda, yadda, yadda”  thought.   The Agile manifesto   It may derive from an occupational hazard; We’re in the software development business and the agile manifesto tends to be our credo. We honestly believe in motivated and self-organized teams, we believe in meritocracy and we acknowledge that requirements are constantly changing, we believe in technical excellence and in the focus that comes from minimizing work in progress - and we believe in retrospectives and continuous reflections as the primary input for continuous learning. We believe that creating value - such as working software - should be the primary measure of progress.   To me - as a manager who tends to recite from the twelve principles of the Agile Manifesto more frequently than most - the whole vision-strategy-tactics needs a different implementation than the traditional rock solid, well communicated approach.   I simply don’t believe in rock solid.   DORITH   The DORITH principle is a different approach. When someone walks up to me and asks for advice, priority or mentoring I tend to reply:      “Do the right thing!”    I feel obliged to not dictate the answer, but instead facilitate that we go and investigate; what is the right thing to do - in the context?   In any context, the right thing to do will obviously be relative to some kind of direction. An obviously good coaching question pops up:      “What are you hoping to achieve?”    Whatever the answer to the question is, it something worth dwelling with for a while, Is it the real thing? What is the rationale behind it? Rationale - that word is one of my all time favorites. It’s definition is “A set of reasons or a logical basis for a course of action or belief”. Back in my time at university when I studied communication I studied with a professor in Philosophy. He’s interpretation of the word was much more poetic: “A person’s rationale is all the ballast that enables that person to resonate, to make decisions”. It’s personal!   So by digging into the underlying, personal rationale, behind why someone is doing something, it suddenly becomes meaningful - to that person.   The answer to your own question lies within your self: You make the right decision, given the context, based on your rationale. - it reads like a poem:      What you don’t have  You don’t need it now  What you don’t know  You can feel somehow     U2, A Beautiful Day   Small Steps Manifesto   It’s obvious that a few lines from a U2 pop song can’t hold a magical answer to a complex problem; how to be a successful manger. But I believe that efficient management implies that people on the team are self-organized, motivated and empowered.   The small steps manifesto is still emerging, but it reads something like:      Any decision that is conscious and based on a rationale, and which is bringing you one small step closer to what you are hoping to achieve, is by definition a good decision.    It’s likely that there might be other decisions someone else could have made, that would have been better in the meaning that they would be even more efficient, but they would have the built-in problem, that they wouldn’t be your’s.   The DORITH principle allows you to learn - it allows you to practice that art of decision making. Find the answer inside yourself, based on your own sense of direction and your own rationale and do something.   It simply allows you to be tactical - relative to where you are headed.   Pivot without mercy or guilt   Making decisions based on your own rationale, however conscious they may be, is bound to have some limitations - for all of us. It’s build into the principle, that some of the decisions will doubtlessly be bad or questionable.   Suck it up!   A proverb that’s often cited in LEAN and agile is “pivot without mercy or guilt”. In it’s essence it captures the remedy for how to deal with bad decision: Simply make another decision. Don’t look back, don’t be sorry. Just make another decision, that’s better.   This of course implies, that we discover when we make bad decisions, and the tools for that are retrospectives and continuous reflections.  ","categories": [],
        "tags": ["LEAN","agile"],
        "url": "/stories/the-dorith-principle/",
        "teaser": "/assets/images/posts/dorith.jpg"
      },{
        "title": "Git aliases",
        "excerpt":"Git aliases are mostly used for nifty shorthand variants or combinations of existing git commands. But aliases can do anything that you can fit into one line - literally. This also makes them fun bash scripting exercises - and inconceivably powerful.   This blog contains quite a few git tricks, tips and even a recommended automated GitOps workflow. It’s specifically detailed in terms of git aliases and utilization of git config. I’ve provided a short Table of Content so you can prepare for what’s coming.   Aliases   Good git aliases are like collectors items. They are one-liners, that makes your life (the git related part of it) much easier. They are literally one line statements. Here’s a short classical example:   git config alias.co 'checkout'   Now I can run git co as a shorthand substitute for git checkout This is a very popular use of git aliases - it saves me six chars every time I want to checkout a branch.   Tab completion   Tab completion is another nifty git feature worth mentioning when the talk is on optimizing your flow in git. In some git distributions it works out of the box, in others it needs a bit of tweaking to be ignited. What it does is simply, that it finishes your git commands for you. If you type the beginning of a git command and then hit &lt;tab&gt; it will toggle through the options you have and when you found the one you like you hit &lt;enter&gt;.   If I use tab completion then it turns our that I had no need for the co shorthand for checkout that I just created; I could already checkout with the same amount of key strokes (6): git co&lt;enter&gt; = git c&lt;tab&gt;&lt;enter&gt;.   “OK! No revolution yet, but hang on a bit, it gets wilder”   Git config   A word on the git config command too: It’s essentially nothing but an in-line ini-file editor/reader of what is in your git configuration files. Git comes with knowledge of three system files; one is local (default) and resides in the repository: /.git/config another one is global and is in your user home folder: ~/.gitconfig, and the last one is system and sits in the /etc/gitconfig folder (I’m using *nix style locations - get an in-depth understanding of config files here).   git config --local  ...  #writes to .git/config in the .git folder 'underneath' your repository git config --global ...  #writes to ~/.gitconfig on your PC git config --system ...  #writes to /etc/gitconfig on your PC   So looking at the consequences of the previous git config command that created the alias it ended up in the .git/config file in the repository that was in the path when I executed it looking like this:   [alias]   co = checkout   …In fact could just have hacked that file directly in my editor, changes will take effect instantly when you save the file - I often find myself editing the config files more often than using git config.   Stepping it up a bit   So let’s find a more useful use of aliases - check this next one. I’m storing useful - but hard to remember - git command (scroll the code block to the right - it’s long!)   git config --global alias.tree 'log --graph --full-history --all --color --date=short --pretty=format:\"%Cred%x09%h %Creset%ad%Cblue%d %Creset %s %C(bold)(%an)%Creset\"'   “Really! That looks complicated!”   Well yes, and no. This is a brilliant example of how aliases really are useful.   The built-in log feature has some beautiful display features, and using the --pretty switch you can draw ASCII art trees of your branches and commits that really are - well artful. But no one wants to write, or even remembers, this relatively complicated format: string. Now I’ve created an alias called tree which since then has become my personal all time favorite git command.   And here’s a few bonuses: First: The tab completion feature has built-in knowledge of my aliases. so git tr&lt;tab&gt; will expand into git tree and second: Since the tree command is a variant of the log command it still supports all the switches that log does. As an example; log supports the switch -&lt;number&gt; to limit the depth of the log to &lt;number&gt; items. So my tree command also supports -&lt;number&gt; out of the box. To see my git tree 32 commits deep I’ll execute:   git tree -32   …and get:                      What you see is the history, 32 commits deep, in the repository that holds the website you are reading. it shows my commit SHA1s, the dates, the branches, the tags, the commit messages and the committers - exactly one line per commit. I don't have many branches in my repo; Since I minimize work in progress and always release from master, but if there were a pile of branches, they would have shown in ASCII art to the left. Tree is now a pretty cool alternative to plain git log -32 and even an alternative to some of the graphical interface gitk.   (click image to open full-size in a new tab)  Git aliases vs git extensions   An alternative to git aliases are git extensions. They behave very much the same way. A git extension is merely an executable - usually a script - that is made accessible from your $PATH and which follows the naming convention that it must be prefixed with git- so if I create the script git-co like this:   #! /bin/bash git checkout $@   And put it in /usr/local/bin - which is already in my $PATH - then I get the exact same result: git co is now shorthand for git checkout. And even git extensions are also automatically supported by tab completion, so as a git user, you wouldn’t know the difference if it was implemented one way or another.   “OK, then why do we have both?”   Well because one is light-weight and the other can hold unlimited amount of complex code. A git alias is limited to one line, that is; It can potentially contain more then one statement (I’ll show you in a bit), but it can not span more than one line. It’s literally a one-liner.   An extension is used when you want to extend git, not just with nifty shorthand variants and cute tricks, but entire landscapes of new features. Git’s graphical display tool called gitk is really a quite extensive (12K+ lines of code) executable called git-gitk written in Tcl/Tk.   Taking aliases to the next level   Let me introduce a nifty feature: Full support of semantic versioning in all your git repositories that might seem quite complex, something that would require a full-blown git extension (and there are quite a handful of them out there on the internet, which proves that the feature is popular and useful) but which turn out, can be actually be implemented in just two lines of code, and stored as aliases.   “No! Really?”  “Yes! Really!”   Implementing support for Semantic Versioning    SemVer Basics   It's essentially a naming convention, a protocol, in which you define version numbers in three levels of integers separated by dots:  &lt;major&gt;.&lt;minor&gt;.&lt;patch&gt;  Then you play by the SemVer rules which goes as follows: When you make a new release, you bump one of the three levels. The semantical meaning of the three levels are as follows: Major is bumped if your release has features that breaks backward compatibility (e.g. a function that used to return a string now returns an integer). A bump of Minor also indicates that your release contains new features, but that they are backward compatible (e.g. the original function that returned a string is untouched, instead a new one is added, which returns an integer). And finally a bump in patch indicates that no new features were added, only bugfixes or enhancements to existing ones.  Another rule in semantic versioning is that if you bump a level, then all other lower levels are reset to zero using 1.2.3 as an example:  Bumping major in 1.2.3 becomes 2.0.0.  Bumping minor in 1.2.3 becomes 1.3.0.  Bumping patch in 1.2.3 becomes 1.2.4.  SemVer's obvious use case is in versioning interfaces or individual component releases, where the protocol lays the foundation of programmatically determining wether or not it's safe to update a given component or not. SemVer is the most important tool in the toolbox, when striving to kill the a bloated monolith system compound into multiple nimble individual component releases.   I will not make this blog about Semantic Versioning (SemVer) in general, but specifically about how to implement it in just two lines of code, using git aliases. So I assume that you’re familiar with the concept - if not read up on it in the short recap in the fact-box.   It’s my belief, that a good workflow is one that is simple and easy to use. Sometimes workflows aren’t simple and easy to use, and in context of git and git related workflows (e.g. GitOps) git aliases (…and git extensions) are a brilliant and obvious way to simplify a workflow so that every team member has a few everyday git favorite git commands which are shared among every team member, and essentially implement the workflow - nice and easy. So before getting to work, I’ll just set the scene and put a few words on the workflow I use and advocate.   I assume the following      I’m working in a Centralized Distributed Workflow This is typically the case for all team using a git-as-a-Service platforms (mine is GitHub but any will do).   The repository has a declarative pipeline defined which monitors certain events in the centralized repository (mine is in Circle CI but any will do).   The declarative pipeline implements “GitOps” That is; When something happens in git, then stuff is automatically verified, promoted or deployed. I have three actions/levels in my repos:            Ready: Any new commit that arrives on a branched prefixed with ready/ is automatically rebased against  master and verified. If the verification is successful it’s automatically merged (guaranteed to be fast-forward, since it’s already rebased) to master       Master: Any new commit that reaches the master branch is automatically verified - and if flawless - deployed to the stage environment.       SemVer: Any commit that is tagged according to SemVer rules is automatically verified and deployed to the production environment.              WTF! - No Pull Requests?     It’s worth noticing that in my workflow I don’t use pull requests. The reason is really that while the generic concept of a Pull Request (PR) undoubtedly exists, it is nevertheless implemented differently in GitHub, GitLab, BitBucket, Azure DevOps, each in its own proprietary implementation. And on top of that  PR’s were originally designed, to support a Benevolent Dictator Governance Model (BDGM) in which only the Benevolent Dictator For Life (BDFL) and the trusted lieutenants have write (push) access, and all other contributors were potentially seen as riff-raff and they would have read (pull) access - but not write access. To contribute they would then have to fork or branch the code, make the code suggestions and then place a Pull Request with one of the trusted lieutenants, who would then read and validate the code for your, and if accepted then they would pull it in. Quite a cumbersome and manual process. And in the light of the fact that in most of todays git repositories, all contributors already have write access, it seems like it’s an obvious thing to optimize.   Let’s have a look at what a pull request is generically and why it may have survived and remained popular in many teams despite almost no one today works in a Benevolent Dictator Governance Model anymore. Generically, a PR is implemented as a short-lived temporary branch related to a specific increment (task) that needs some level of verification before merged into another branch - usually master. This is no different than the flow I advocate, where we say, that any code change must be done on a separate short-lived branch and only accepted into master if quality measures are sufficiently met. So consequently, my ready/ branch is like a pull request, It’s simply a way to signal to my automated GitOps backend that I’m ready to take the test and see if my code is worthy of being integrated onto master. The advantage I get is that I’m only git native features - in this case simply a dedicated short-lived branch and a self-made naming convention. So consequently my setup is compliant with any Distributed Centralized Git repository strategy - including all the popular git-aaS-platform mentioned earlier.      Nah! You’re missing out on reviews!    Yes, since I’ve taken PR’s out of the equation what I often get is: “Hey, you can’t do that, what about peer reviews then?”   And it’s true that in most teams Pull Requests are associated with, or sometimes even a synonym to, code reviews. But in my world - if a code review is required - then it’s performed directly on the short lived issue branch - all the different git-aaS-platforms have perfect built-in support for annotated reviews. It’s easy! And it has the desired side-effect that getting an extra set of eyes on the code in the development process turns it into a collaboration process among peers. And remember: Peers are equals! They are not supposed to be each others gate-keepers. The flow I advocate has a sent of paired programming among peers or the same flow can be used in a lean Senpai and kōhai inspired relation where a master takes responsibility for an apprentice.   The choice to ditch reviews as quality gates and to only use native git commands as opposed to proprietary implementations of Pull Requests has to do with my general admiration of the concept of lean and automation. A review, regardless if it’s done by a peer or a mentor, can’t possibly be automated, since it requires a reel persons sincere opinion. Essentially this makes the review belong to a validation process as opposed to a verification process. For that reason it can not possibly become part of the process that’s supposed to run automatically - as it would inevitably introduce a wait state - an in lean processes waiting is considered waste. I’m not adding waste to my flow - I’m removing it!   My workflow is simply workon and deliver   When a task is assigned to me (I’ll use issue #34 as an example), I create a shot-lived branch for the task and implement my solution. The way I do this is to run git workon 34 but that’s because I’m using a git extension that supports git workon I’ll leave that for a later blog post. I’t essentially roughly the same as:   git checkout master git pull git checkout --track origin/master -b 34-my-short-lived-issue-branch   I’m now on branch 34-my-short-lived-issue-branch and I then start hacking my solution. If I need feedback or an OK from a peer or a mentor before I can deliver to the automated pipeline then I’ll commit with a  mention of my colleague and push is to the centralized repository (origin) - like this:   git commit -m \"Hey @DenverCoder32 - please look at his\" git push origin 34-my-short-lived-issue-branch   If I’m on any of the popular git-aaS-platforms - then the mention of my colleague in the commit message will trigger that she gets a notification with a link to the commit, and she can go and annotated her comments or review on my commit, exactly as if it was one of the proprietary pull request implementations.   When I’ve gotten the feedback from my peer or mentor - or if I don’t need it - then I deliver to the automated pipeline simply by pushing the branch again - I have a git extension installed allowing me to do git deliver it will simply prepend the branch with ready/ in the push process - it plays roughly something like this:   git push origin 34-my-short-lived-issue-branch ready/34-my-short-lived-issue-branch   The name ready/ triggers my pipeline. If all is well then the rest is really as automated - and boring to watch - as a washing machine wash.   Using Circle CI as an example, here’s how my pipeline is defined:   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 - deliver:     requires:       - prep-repo       - jekyll-build     filters:       branches:         only:           - /ready\\/.*/ - stage-deploy:     requires:       - prep-repo       - jekyll-build       - html-proofer     filters:       branches:         only:           - /master/ - prod-deploy:     requires:       - prep-repo       - jekyll-build       - html-proofer     filters:       branches:         ignore: /._/       tags:         only: /^._\\d+\\.\\d+\\.\\d+.*$/ # Contains a Semantic Version number   In case you’re not that familiar with Circle CI but are more proficient in one of the many other declarative pipeline technologies I’ll give you a quick run-down of what’s going on in the YAML snippet above so you can do something similar in GitHub Actions, Azure Pipelines, GitLab CI, Jenkins - or whatever declarative pipeline you are using - they all support something similar.   I have defined three alternative paths in my flow. All stages apparently do the same two steps to begin with (lines 3+4, 11+12 and 20+21). prep-repo rebase from and then fast-forward merges the commit into master if it isn’t there already and generates a version.txt file, which I can include in the release. It then caches the repo on Circle CI infrastructure for potential reuse. jekyll-build builds the static website with Jekyll and caches only the derived HTML - that is the result not the source - again for potential reuse. The results from these various stages are cashed with reference to the SHA1 of the commit, so if I re-run the same commit, the two stages are reused rather than replayed.   In addition to prep-repo and jekyll-build the two deploy stages also requires a flawless html-proofer (lines 13+22).   The deliver flow is triggered only by branches that are prefixed with ready/ (lines 6-8). The stage-deploy flow is trigger only by the master branch (lines 15-17) and finally then prod-deploy stage is triggered not by branches, but only tags that contains a SemVer pattern (lines 24-27).   So the ready/ branch from before triggered the deliver flow, which ended up merging onto master which then in turn triggered the stage-deploy flow, which reused the prep and build and in addition checked for dead links and if successful automatically deployed to my stage environment, which is a real-time live production-like environment.   This is my own website, but had it been a product with a customer/product owner who needed to validate before deploying to production, then I could point them to the updated stage environment and wait for their approval and simply set a SemVer tag and push it. It could look like this:   git tag -a -m \"Making a patch bump on 1.2.3\" v1.2.4 git push --tags   This would trigger the prod-deploy workflow - which would again reuse the prep and build steps and then deploy to production being the website you’re reading now.   And this is where I now need a tool to help me              When I run the git tag command it lists all the tags I have in my repo - but I'm really only interested in one with the highest SemVer value.  To get my next SemVer tag, regardless of what level I want to bump, I need to know the previous level. I do:   git tag   And I would then seek out the one with the highest SemVer value - in this case it would be 1.2.4 which I can tell from the rather inconsistent use of naming in tag version_1.2.4_some_comment. What really could be helpful would be a git semver command which simply replies 1.2.4.   It turns out that this is quite easy to get from a shell command like This:   git tag | grep -Eo '\\d\\.\\d\\.\\d' | sort | tail -1;   OK, not very complicated, but still complicated enough to be cumbersome to type each time I need it.   Storing bash scripts as one-line git aliases - using closures   If the alias expansion is prefixed with an exclamation point, it will be treated as a shell command.   So I could create a semver alias like this:   git config --global alias.semver \"\\!git tag | grep -Eo '\\d+\\.\\d+\\.\\d+' | sort | tail -1\"   But git aliases also automatically passes an any parameter to the execution. This is sometimes desired, as we saw in the tree alias previously, where the switch supported by git log was also automatically supported by git tree.   But sometimes it’s not desired - sometimes I don’t accept parameters or switches, and I want them to be swallowed or ignored and sometimes (as you’ll see later) I want to pass parameters or switches to the alias itself, not it’s execution.   So the semver implementation above would mean that an execution like git semver some_rubbish_parameter would make it fail, whereas I would like the rubbish parameter to be ignored.   The trick to achieve this is instead of defining the execution, we define a function, that does the execution, and then we execute the function - a trick that’s know as a closure. This way we get to manipulate or investigate any input before it’s unconditionally executed   The closure construct in bash looks like this:   #template f() {statement-1; statement-2; ...; statement-n; }; f  # semver as closure f() { git tag | grep -Eo '\\d+\\.\\d+\\.\\d+' | sort | tail -1; }; f   But before we store it, let’s consider, what happens if there aren’t any SemVer tags set yet, if we’re the first developer here. Aaarh! Nothing is returned, so bumpsemver has nothing to bump. Let’s fix that. Some teams wants to start the SemVer with 0.0.0 others with 0.9.0 or 1.0.0. Let’s store the team’s initial preference in the config file in a new section called [semver]`- something like:   git config --global semver.initial 0.0.0   And then let semver return that if the grep command doesn’t catch anything. OK! One down - git semver is in the box - it ended up like this:   git config --file `git root`/.gitconfig alias.semver   \"\\!f() { SEMVER=\\`git tag | grep -Eo '\\\\d+\\\\.\\\\d+\\\\.\\\\d+' | sort | tail -1\\`; if [ '_' == _\\$SEMVER ]; then echo \\`git config --get semver.initial\\`; else echo \\$SEMVER; fi; }; f\" # &gt;&gt; Scroll to the right - it's long &gt;&gt;   Now it always gives me the current SemVer, regardless of how inconsistently I named my tags. And if there’s no match I gets the number from the config and returns that .   Getting a bit ugly - adding support for parameters and switches   Next up is a git bumpsemver command which I’d like to take one of three possible switches, indicating which level to bump. I would then like it to type out - but not execute - the command that I would use to  the next SemVer label. Having it type out rather than execute it means that it would always be safe to (test) run the command, and If I like it, I can just rerun it inside an eval $( ...) construct.   As show in one of the previous examples, when I set my SemVer tags I always make them annotated and I always provide a message, usually I mention what the previous SemVer was - so keep a bit of history something like:   git tag -a -m \"minor bump on 1.2.3\" v1.3.0   But others might want to apply their own messages. So my decision is, that I need git bumpsemver to take an additional but optional parameter, which will become the message to -m. If no message is applied, then bumpsemver should automatically generate something similar to the example above.   I’ve also seen many software teams prefix their SemVer with v or ver like v1.3.0 or ver1.3.0 so I wan’t to have support for that too. I’ll create a prefix setting for this and put it in the in semver section. So if you want a prefix v it should be set like this:   git config --global semver.prefix v   Which would total the semver section in the config file to look like this:   [semver]   inital = 0.0.0   prefix = v   And finally, on top of everything, I’d like the alias to be so clever, that if I execute it wrongly, it does nothing wrong, but instead it just shows a nice and short instruction on how to use it correctly something like:   $ git bumpsemver Usage: git bumpsemver  --major|--minor|--patch [msg]  Generates the git command to run. If you omit the [msg] a clever one will be generated for you. To execute it run it in an eval $(...) like this example:  eval $(git bumpsemver --minor \"this will be the comment\")   That’s what I want, nothing more, nothing less. All in just one line of code - as a git alias.   It’s not as daunting a task as it seems. First I’ll lay it out as it looks in a neat bash closure, and then we’ll weed out the new-lines and escape whatever needs to be escape for persistent file storage. I give you git bumpsemver dressed up as a closure:   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 f(){   PREFIX=$(git config --global --get semver.prefix);   if [ -z \\\"$2\\\" ];     then MSG=\"-m \\\"$1 bump on `git semver`\\\"\";     else MSG=\"-m  \\\"$2\\\"\";   fi;   levels=(`echo \\$(git semver) | tr '.' ' '`);   if [ '_--major' ==_$1 ];     then echo git tag -a $MSG $PREFIX$((${levels[0]}+1)).0.0;   elif [ '_--minor' == _$1 ];     then echo git tag -a $MSG $PREFIX${levels[0]}.$((${levels[1]}+1)).0;   elif [ '_--patch' == _$1 ];     then echo git tag -a $MSG $PREFIX${levels[0]}.${levels[1]}.$((${levels[2]}+1));     else echo 'Usage: git bumpsemver  --major|--minor|--patch [msg]\\n       Generates the git command to run. If you omit       the [msg] a clever one will be generated for you.       To set the tag run bumpsemver inside an eval $(...) like this example:           eval \\$(git bumpsemver --minor \\\"this will be the comment\\\")';   fi; }; f\"   There’s quite a few cheap bash tricks in play here - so if you are curious to what happens I’ll walk you through it.   Line 2: As mentioned earlier, git config is also a reader. So the script simply reads whats in the file ~/.gitconfig file.   Lines 3-5: The -z switch to the file test is TRUE if the variable is not defined. If no messages is applied then a default one is generated, if a message is given then it overrides the default message - mentioning the current SemVer level by reusing the git semver alias.   Line 7: tr replaces dots with spaces - and returns an array. tr splits on spaces so 1.2.4 becomes a three item array [1][2][4] and an arrays is what is needed in (lines 9, 11 &amp; 13). Also notice, that bumpsemver is reusing the semver alias.   Lines 8,10 and 12: A dummy _ char is added in the comparison on both sides, otherwise the script would fail if no parameters were applied - comparing something to noting, and we want that failure to be a friendly usage statement in line 14.   Lines 9,11 and 13: Type casting $levels with ${...} allows us to access the array items. And type casting the sting to an integer using s((...)) allows us to use arithmetic operators, so we can increment the level.   Now let’s wrap this up - it’s a bit ugly, but we have to fit it onto one line - be warned:   git config --global alias.bumpsemver \"\\!f(){ PREFIX=\\$(git config --global --get semver.prefix); if [ -z \\\"\\$2\\\" ]; then MSG=\\\"-m \\\\\\\"\\$1 bump on \\`git semver\\`\\\"\\\\\\\"; else MSG=\\\"-m  \\\\\\\"\\$2\\\\\\\"\\\"; fi; levels=(\\`echo \\$(git semver) | tr '.' ' '\\`); if [ '_--major' == _\\$1 ]; then echo git tag -a \\$MSG \\$PREFIX\\$((\\${levels[0]}+1)).0.0; elif [ '_--minor' == _\\$1 ]; then echo git tag -a \\$MSG \\$PREFIX\\${levels[0]}.\\$((\\${levels[1]}+1)).0; elif [ '_--patch' == _\\$1 ]; then echo git tag -a \\$MSG \\$PREFIX\\${levels[0]}.\\${levels[1]}.\\$((\\${levels[2]}+1)); else echo 'Usage: git bumpsemver  --major|--minor|--patch [msg]\\n\\nGenerates the git command to run. If you omit\\nthe [msg] a clever one will be generated for you.\\nTo execute it run it in an eval $(...) like this example:\\n\\n    eval \\$(git bumpsemver --minor \\\"this will be the comment\\\")'; fi;  }; f\" # &gt;&gt; scroll - it's very loooong &gt;&gt;   Git Config team collaboration - fixing the missing level in git config   OK, now I’ve given you an handful of useful git aliases, and git config settings. Now it would be nice, if everyone on the team, had these settings and aliases, then we would know that every team member was doing the same thing. As a team lead I could ask everyone  to execute all the commands that would set these aliases. Or distribute a script that did it, but I admit, that the commands are delicate, they are not even white-space tolerant, the removal of just one tiny white-space might render the alias useless. And personally I favour the principle of Configuration as Code  which mean that I would much rather distribute and version control a config file, as opposes to a script that sets the config file.   But there is kind of a missing level in the whole git config story. --system and --global levels are on the developers own PC and out of reach for version control. --local is in the repository’s .git folder which is kinda underneath it all and not included in the version control and general clone, pull, push, branch scope.   I kinda miss a fourth level in the whole git config setup, something like --repo that would get and set from a file .gitconfig which was located in the root of the repository, and then could be version controlled together with the rest of the team’s files.   And then I need git config to include this new fourth repo level in the equation.   First part is easy to achieve since the git config already supports that I can use the --file switch to write to any other file. So I could do something like:   git config --file &lt;repository-root&gt;/.gitconfig alias.co checkout   Hmmm, looks like all I need now is a generic way to get the repository root, something like git root. Such a git command doesn’t exist, but I can get the root with git rev-parse --show-toplevel and I can easily make it a git root command if I want:   git config --global alias.root 'rev-parse --show-toplevel'   Second part is to include the new config file in the git config hierarchy so it’s settings comes into play, I can use the include directive supported by git config it’s not 100% ideal, since I would have to put the include in one of the three default config files, and as discussed none of them are version controlled but it will work. Relative paths in the include directive takes off-set in the location of the config file itself, the --system and --global levels can’t really act as generic in regards of the repository location, as I may have cloned the repos to anywhere on my hard drive. So for this reason it turns out that the --local level has one advantage over the two other, as it always sits underneath the repository which mean that the repository level .gitconfig is always in the parent directory, relative to the local config file (remember it’s in the repository’s .git/config). So every git repository on earth, regardless of where it cloned to, has a valid --local include path that looks - like this.   git config --local include.path ./../.gitconfig   So if the team distributes their shared aliases and config settings in general in a .gitconfig in the root of the repository, where they probably already have all the other config files they share. Then the git config command above would then be the only command I would require for all teammates to run the same settings. Using the local level implies that it must be included and set in every repository. So the final alias I give you is one that reads everything in the repo’s .gitconfig and pours it into the --global and essentially make them all your own. It actually work as intended, since the local level is read last, and the include level is read as the very last. So any updates your team makes in the repo, would take precedent over your global ones. I’ve made the alias safe, so it won’t update your global settings if the are already defined.   git config --global alias.repo-config-to-global \"!f(){ for f in $(git config --file `git root`/.gitconfig --list --name-only); do git config --global --get $f &gt; /dev/null || git config --global $f \"$(git config --file `git root`/.gitconfig --get $f)\"; done; }; f\"   If you want to use git repo-config-to-global to update existing settings, you do it by deleting them first and reapplying them from the one in the repository:   for setting in $(git config --file `git root`/.gitconfig --list --name-only); do git config --global --unset $setting; done; git rep-config-to-global # &gt;&gt; Scroll to the right - it's long &gt;&gt;   All in All - the final config file   It ended up like this:   [alias]   tree = log --graph --full-history --all --color --date=short --pretty=format:\\\"%Cred%x09%h %Creset%ad%Cblue%d %Creset %s %C(bold)(%an)%Creset\\\"   semver = \"!f() { SEMVER=`git tag | grep -Eo '\\\\d+\\\\.\\\\d+\\\\.\\\\d+' | sort | tail -1`; if [ '_' == _$SEMVER ]; then echo `git config --get semver.initial`; else echo $SEMVER; fi; }; f\"   bumpsemver = \"!f(){ PREFIX=$(git config --global --get semver.prefix); if [ -z \\\"$2\\\" ]; then MSG=\\\"-m \\\\\\\"$1 bump on `git semver`\\\"\\\\\\\"; else MSG=\\\"-m  \\\\\\\"$2\\\\\\\"\\\"; fi; levels=(`echo $(git semver) | tr '.' ' '`); if [ '_--major' == _$1 ]; then echo git tag -a $MSG $PREFIX$((${levels[0]}+1)).0.0; elif [ '_--minor' == _$1 ]; then echo git tag -a $MSG $PREFIX${levels[0]}.$((${levels[1]}+1)).0; elif [ '_--patch' == _$1 ]; then echo git tag -a $MSG $PREFIX${levels[0]}.${levels[1]}.$((${levels[2]}+1)); else echo 'Usage: git bumpsemver  --major|--minor|--patch [msg]\\\\n\\\\nGenerates the git command to run. If you omit\\\\nthe [msg] a clever one will be generated for you.\\\\nTo execute it run it in an eval  like this example:\\\\n\\\\n    eval $(git bumpsemver --minor \\\"this will be the comment\\\")'; fi;  }; f\"   root = rev-parse --show-toplevel   repo-config-to-global = \"!f(){ for f in $(git config --file `git root`/.gitconfig --list --name-only); do git config --global --get $f &gt; /dev/null || git config --global $f \\\"$(git config --file `git root`/.gitconfig --get $f)\\\"; done; }; f\" [semver]   prefix = v   initial = 0.0.0   How to install   You have options:   Copy to config file      Simply copy the content from the final config file shown above into your own - into your own config file   Run from terminal      Clone my lakruzz/semver_git_alias repo   Change directory to be inside the repo   Include the .gitconfig file from the repo   Pour them into you own global level   Copy and run theses four lines:   git clone https://github.com/lakruzz/semver_git_alias.git cd semver_git_alias git config --local include.path ./../.gitconfig git repo-config-to-global   I’ve put everything in a small git repository lakruzz/semver_git_alias. If you have any comments or want to share some of your own git aliases, then let’s chat up using the GitHub issues.  ","categories": [],
        "tags": ["git","gitops","semver"],
        "url": "/stories/git-aliases-extreme/",
        "teaser": "/assets/images/posts/octocat_alias.png"
      },{
        "title": "The Full Assessment",
        "excerpt":"   This model originates from the time when I founded and operated Praqma. In this post, I’ll describe how I conducted this Continuous Delivery Metric Assessment at more then 25 companies   The name and concept “Continuous Delivery” is a deliberate reference to a lean or TPS inspired approach often used in manufacturing which takes offset in a one-piece flow that strives for quality built into the product as opposed to glued on - after it’s done.   In the modern world of software development we use the concepts Continuous Delivery and DevOps to be the equivalents in software to what lean means to manufacturing.   We strive for Lean Software Development.   Value Stream Mapping - the as-is picture   In Lean manufacturing there is a strict focus on Product Lifecycle Management – in software our focus is on Software Development Lifecycle Management (SDLC).   As in lean, we investigate and assess three different core phenomena that jeopardize an ideal delivery value stream:      Muda: Wastefulness – things and processes that are excess, which don’t contribute to value.   Muri: Unevenness – inefficient coordination of takt and workflows that build up stock or create wait-states.   Mura: Overburdenness – bad utilization of resources, scope creep and poor prioritization   Just as in a lean process we get the first impression of the current state from an engaging workshop exercise based on a Value Stream Mapping technique. We gather team members and stakeholders and facilitate them to draw a rich picture of the current (as-is) process. We mark the waste, the back-loops, the wait states, the excess manual processes, the communication conflicts etc.   The Value Stream Mapping exercise’s overall purpose is to understand current workflow and most importantly map lead times. We map the DORA metrics; Deployment Frequency, Mean Lead Time for Changes, Mean Time to Recovery, Change Failure Rate and reliability.   I’ve gathered a handful af pictures from some of the workshops and the first pile is from the initial part of the session where we do a Value Stream Map. It’s a rich picture we draw on the wall, everyone involved in the process has a pen, and we’re mapping what exactly goes on while the software is produced, validated and shipped.                                                                                                                                                                                                                                                                                        The workshop starts with mapping out the Value Stream of the product as rich pictures laid our on the walls       When the Value Stream begins to take form we introduce the trouble markers which the participants use to mark where the process needs to be optimization in six different categories of troubles:         Manual Work - dull, repetitive work done my humans   Queues - processes where takt is off due to lack of resources   Wait states - processes where flow is broken - often during handovers. Tasks are dropped between chairs. What is important to someone is unimportant to someone else   Conflicts - parties fighting each other, serving different and contradicting agendas   Unplanned work - prioritization and systemic approach is soo poorly adapted, that plans fall apart all the time. It’s fire fighting   Needs repair - Processes are adapted to a certain situation, but the world has moved on. These processes needs to be updated   Some board examples                                                                                                                                The mappings show the troubles in the process and and all the painful back-loops       When the Value stream is mapped and assessed we ask the team to prepare a walk-through of what goes on in the board. One person will start in the upper left corner and explain all the processes, troubles, connection, and back-loops.   It’s very seldom that there exists such one person that has the full overview of the entire process. During the walk-through new team members will have to take over. Most often no one grasps the full picture.                                                                                                                                                                      The mappings show the troubles in the process and and all the painful back-loops       The Goal - get the full picture?   The Dreyfus brothers published a model for skill acquisition which contributed to the field of Cognitive Science as early as in the 80’s to propose an early model for understanding intelligence and expertise.  This has inspired us to build a model, not necessarily – or not just – for maturity, but also for in-depth metrics.   In Continuous Delivery in context of software development we focus on different areas of knowledge or matter; Build, Test, Version control, DevOps, Architecture &amp; Design and Organization &amp; Culture. These areas can be molded and adapted to the individual client and assessment.   After the Value Stream mapping session we present the model and ask the team to score each card from a miraculous point of view:      Imagine that the principle described on the card was fully implemented in your process. How much would it affect throughput, feedback and payback — and how complex would the implementation be    The model suggests different approaches, techniques, tools or principles, which represent different levels of expertise and proficiency. We use the model to conduct a facilitated discussion with the team and stakeholders. Through this workshop we will debate every single card and assess how relevant this capability is in context of the quality of the product and the Software Development Lifecycle.   So we spread out in smaller teams and each team picks up a couple of cards of their own choice. Each card is scored for effect and cost of implementation. The processes continues until all cards are scored and assessed. The team must agree and debate the scores to get to some level on consensus.   Scoring the cards in the model                                                                                                                                                                                                                                                                                        The workshop starts with mapping out the Value Stream of the product       This concludes the first day of the Value Steam Mapping - and Card scoring workshops.   Observations – digging in the dirt   Next stage is to get our fingers into the dirt and have various team members explain and demonstrate some of the tools and processes that we identify as a special point of interest. These sessions will be scheduled when we have the overview established from the initial workshops, so we can say for sure what these points of special interests are but typically we would do a combination of code inspections, static analysis, review tool utilizations, and we will study and observe the team’s common ways of working, branching strategies, deliveries, deploys, monitoring etc.   Lean Coffee Workshops   To prepare ourselves to go into solution mode and enable us to suggest a feasible and realistic roadmap towards the end goal we will organize a series of “lean coffee workshop” - typically one in each area in the model – and whatever else is needed. In these workshops we will invite team members and stakeholders with special expertise in the field.   A lean coffee workshop is a special workshop format that allows participants to help build that agenda of the workshop dynamically in such a way that dire problems or issues take higher priority. For each discussion topic we will facilitate that we capture:      The root cause problem   A possible (maybe more) solution to mitigate it   The value each identified solution would yield to SDLC or product quality   The findings are then recorded and presented as a detailed and customized note on each card - see card “Automated Builds” as an example (these are real finding from some of the assessments, but anonymized and used here to serve as examples). Also notice on the “Automated Builds” card, that the scoring from the workshop is captured. The score is 7 and complexity is categorized as a group effort.   The report   This concludes our onsite effort – from here we will take the elicited intelligence home to discuss internally among ourselves and we will write up a report and propose a prioritized list of all findings in. It’s a quite comprehensive report that can serve as a roadmap or backlog for the work ahead.   The report will include an executive summary.   If you are interested the model score, the findings and the prioritized recommendations can be delivered to you as a static website; your one version of the website that your’re currently at, which you can host securely on your own corporate inside – for every stakeholder to browse and reference.   This framework and model are inspired by lean principles, cognitive science, DORA metrics and DevOps and agile culture. Lars Kruse has been the lead on the development of this model and approach and has conducted more then 25 assessments based on this framework including:   Grundfos, Danfoss, Universal Robots, Volvo, Terma, Brüel &amp; Kjær Sound &amp; Vibration, Atmel (now Microchip), ATP, Alm. Brand, 3Shape, TetraPak, Novelda, Yxlon International, Oticon, Kamstrup, Napatech, Schneider Electrics, Schlumberger, Cryptera, DR, BK Ultrasound.  ","categories": [],
        "tags": [],
        "url": "/stories/the-full-assessment/",
        "teaser": "/assets/images/pics/ws-0025.jpg"
      }]
